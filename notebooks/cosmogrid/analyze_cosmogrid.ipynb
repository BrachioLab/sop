{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc45f577-1f7b-43d1-981b-bd2077a6d07b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# google/vit-base-patch16-224-in21k\n",
    "\"\"\"\n",
    "python actions/vanilla/train_vanilla_imagenet.py --need-cls\n",
    "\"\"\"\n",
    "import torch\n",
    "# from transformers import ViltProcessor, ViltForQuestionAnswering, ViltConfig\n",
    "import os\n",
    "import argparse\n",
    "# import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import ViTImageProcessor, AutoModel\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "import matplotlib\n",
    "from skimage import measure\n",
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "\n",
    "\n",
    "def resize_image_masks(image, masks):\n",
    "    num_channel, img_dim1, img_dim2 = image.shape\n",
    "    masks = masks.int()\n",
    "    new_masks = []\n",
    "    new_images = []\n",
    "    for i, mask in enumerate(masks):\n",
    "        non_zeros = torch.nonzero(masks[i])\n",
    "        if non_zeros.size(0) == 0:\n",
    "            continue\n",
    "        min_x = torch.min(non_zeros[:, 0])\n",
    "        max_x = torch.max(non_zeros[:, 0])\n",
    "        min_y = torch.min(non_zeros[:, 1])\n",
    "        max_y = torch.max(non_zeros[:, 1])\n",
    "        resized_mask = resize(masks[i][min_x:max_x, min_y:max_y].cpu().numpy(), (img_dim1, img_dim2), preserve_range=True)\n",
    "        thresholded_mask = (torch.tensor(resized_mask) > 0.5).int()\n",
    "        new_masks.append(thresholded_mask)\n",
    "        resized_image = resize(image[:, min_x:max_x, min_y:max_y].cpu().numpy(), \n",
    "                                (num_channel, img_dim1, img_dim2), \n",
    "                                preserve_range=True)\n",
    "        new_images.append(torch.tensor(resized_image))\n",
    "    return torch.stack(new_images).to(image.device), torch.stack(new_masks).unsqueeze(1).to(masks.device)\n",
    "\n",
    "\n",
    "def convert_idx_masks_to_bool(masks):\n",
    "    \"\"\"\n",
    "    input: masks (1, img_dim1, img_dim2)\n",
    "    output: masks_bool (num_masks, img_dim1, img_dim2)\n",
    "    \"\"\"\n",
    "    unique_idxs = torch.sort(torch.unique(masks)).values\n",
    "    idxs = unique_idxs.view(-1, 1, 1)\n",
    "    broadcasted_masks = masks.expand(unique_idxs.shape[0],\n",
    "                                     masks.shape[1],\n",
    "                                     masks.shape[2])\n",
    "    masks_bool = (broadcasted_masks == idxs)\n",
    "    return masks_bool\n",
    "\n",
    "\n",
    "def transform(image, processor=None):\n",
    "    # Preprocess the image using the ViTImageProcessor\n",
    "    image = image.convert(\"RGB\")\n",
    "    if processor is not None:\n",
    "        inputs = processor(image, return_tensors='pt')\n",
    "        return inputs['pixel_values'].squeeze(0)\n",
    "    else:\n",
    "        return np.asarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba5e6ade-c37e-42df-b4b1-d8a7668f5280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sum_weights_for_unique_masks(masks, masks_weights, preds): #, poolers):\n",
    "    # Convert each boolean mask to a unique string of 0s and 1s\n",
    "    mask_strs = [''.join(map(str, mask.bool().int().flatten().tolist())) for mask in masks]\n",
    "    img_size = 66\n",
    "\n",
    "    # Dictionary to store summed weights for each unique mask\n",
    "    unique_masks_weights = {}\n",
    "    unique_masks_preds = {}\n",
    "    unique_masks_count = {}\n",
    "    unique_masks_dict = {}\n",
    "\n",
    "    for i, (mask_str, weight, pred) in enumerate(zip(mask_strs, masks_weights, preds)):\n",
    "        if mask_str in unique_masks_weights:\n",
    "            unique_masks_weights[mask_str] += weight\n",
    "            unique_masks_preds[mask_str] += pred\n",
    "            unique_masks_count[mask_str] += 1\n",
    "        else:\n",
    "            unique_masks_dict[mask_str] = masks[i]\n",
    "            unique_masks_weights[mask_str] = weight\n",
    "            unique_masks_preds[mask_str] = pred\n",
    "            unique_masks_count[mask_str] = 1\n",
    "\n",
    "    # Convert dictionary keys back to boolean masks\n",
    "    unique_keys = sorted(unique_masks_weights.keys())\n",
    "    unique_masks = [unique_masks_dict[key] for key in unique_keys]\n",
    "    summed_weights = [unique_masks_weights[key] for key in unique_keys]\n",
    "    mean_preds = [unique_masks_preds[key] for key in unique_keys]\n",
    "\n",
    "    return unique_masks, summed_weights, mean_preds #, mean_poolers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa750d4-c88b-4afb-be37-1516a5071afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/shared_data0/weiqiuy/explainable_attn/exps/cosmogrid_wrapper_lr0005_phs32_watershed_diagonal_os16/best/val_results'\n",
    "\n",
    "X = []\n",
    "images = []\n",
    "mask_paths = []\n",
    "mask_idxs = []\n",
    "preds_all = []\n",
    "labels_all = []\n",
    "mask_id_feat_dict = {}\n",
    "\n",
    "\n",
    "count = 0\n",
    "for filename in tqdm(sorted(os.listdir(input_dir))):\n",
    "    data = pickle.load(open(os.path.join(input_dir, filename), 'rb'))\n",
    "    \n",
    "    image = data['image']\n",
    "    image_array = np.array(image)[0]\n",
    "    masks_used = data['masks_used'].cpu().numpy()\n",
    "    mask_weights = data[\"mask_weights\"]\n",
    "    pred = data['outputs_avg']\n",
    "    preds = data['outputs']\n",
    "    original_preds = data['outputs_original']\n",
    "    label = data['label']\n",
    "    \n",
    "    unique_masks, summed_weights, mean_preds = sum_weights_for_unique_masks(torch.tensor(masks_used), \n",
    "                                                                                          torch.tensor(mask_weights), \n",
    "                                                                                          torch.tensor(preds))\n",
    "\n",
    "    for i in range(len(unique_masks)):\n",
    "        mask_id_feat_dict[(filename, i)] = (image, \n",
    "                                            label,\n",
    "                                            unique_masks[i],\n",
    "                                            summed_weights[i], \n",
    "                                            mean_preds[i],\n",
    "                                           )\n",
    "    count += 1\n",
    "    if count > 1000:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
