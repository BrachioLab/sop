{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7838f6-1134-4508-ab0c-63e9b48b99cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import sys\n",
    "sys.path.append('../../lib/exlib/src')\n",
    "# from exlib.modules.sop import SOPImageCls, SOPConfig, get_chained_attr, get_inverse_sqrt_with_separate_heads_schedule_with_warmup\n",
    "from exlib.datasets.cosmogrid import CosmogridDataset, CNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a96faf6f-204b-42ea-a365-a40d04edc77d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: optimize\n",
    "\n",
    "from __future__ import division\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from transformers import PreTrainedModel\n",
    "import collections.abc\n",
    "from functools import partial\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from transformers import PretrainedConfig, PreTrainedModel\n",
    "import copy\n",
    "import json\n",
    "from collections import namedtuple\n",
    "import os\n",
    "\n",
    "# from transformers import PreTrainedModel\n",
    "\n",
    "\n",
    "AttributionOutputSOP = namedtuple(\"AttributionOutputSOP\", \n",
    "                                  [\"logits\",\n",
    "                                   \"logits_all\",\n",
    "                                   \"pooler_outputs_all\",\n",
    "                                   \"masks\",\n",
    "                                   \"mask_weights\",\n",
    "                                   \"attributions\", \n",
    "                                   \"attributions_max\",\n",
    "                                   \"attributions_all\",\n",
    "                                   \"flat_masks\",\n",
    "                                   \"grouped_attributions\"])\n",
    "\n",
    "\n",
    "def convert_idx_masks_to_bool(masks):\n",
    "    \"\"\"\n",
    "    input: masks (1, img_dim1, img_dim2)\n",
    "    output: masks_bool (num_masks, img_dim1, img_dim2)\n",
    "    \"\"\"\n",
    "    unique_idxs = torch.sort(torch.unique(masks)).values\n",
    "    idxs = unique_idxs.view(-1, 1, 1)\n",
    "    broadcasted_masks = masks.expand(unique_idxs.shape[0], \n",
    "                                     masks.shape[1], \n",
    "                                     masks.shape[2])\n",
    "    masks_bool = (broadcasted_masks == idxs)\n",
    "    return masks_bool\n",
    "\n",
    "\n",
    "def get_mask_transform(num_masks_max=200, processor=None):\n",
    "    def mask_transform(mask):\n",
    "        seg_mask_cut_off = num_masks_max\n",
    "        # Preprocess the mask using the ViTImageProcessor\n",
    "        if len(mask.shape) == 2 and mask.dtype == torch.bool:\n",
    "            mask_dim1, mask_dim2 = mask.shape\n",
    "            mask = mask.unsqueeze(0).expand(3, \n",
    "                                            mask_dim1, \n",
    "                                            mask_dim2).float()\n",
    "            if processor is not None:\n",
    "                inputs = processor(mask, \n",
    "                                do_rescale=False, \n",
    "                                do_normalize=False,\n",
    "                                return_tensors='pt')\n",
    "                # (1, 3, 224, 224)\n",
    "                return inputs['pixel_values'][0][0]\n",
    "            else:\n",
    "                return mask\n",
    "        else: # len(mask.shape) == 3\n",
    "            if mask.dtype != torch.bool:\n",
    "                if len(mask.shape) == 2:\n",
    "                    mask = mask.unsqueeze(0)\n",
    "                mask = convert_idx_masks_to_bool(mask)\n",
    "            bsz, mask_dim1, mask_dim2 = mask.shape\n",
    "            mask = mask.unsqueeze(1).expand(bsz, \n",
    "                                            3, \n",
    "                                            mask_dim1, \n",
    "                                            mask_dim2).float()\n",
    "\n",
    "            if bsz < seg_mask_cut_off:\n",
    "                repeat_count = seg_mask_cut_off // bsz + 1\n",
    "                mask = torch.cat([mask] * repeat_count, dim=0)\n",
    "\n",
    "            # add additional mask afterwards\n",
    "            mask_sum = torch.sum(mask[:seg_mask_cut_off - 1], dim=0, keepdim=True).bool()\n",
    "            if False in mask_sum:\n",
    "                mask = mask[:seg_mask_cut_off - 1]\n",
    "                compensation_mask = (1 - mask_sum.int()).bool()\n",
    "                mask = torch.cat([mask, compensation_mask])\n",
    "            else:\n",
    "                mask = mask[:seg_mask_cut_off]\n",
    "\n",
    "            if processor is not None:\n",
    "                inputs = processor(mask, \n",
    "                                do_rescale=False, \n",
    "                                do_normalize=False,\n",
    "                                return_tensors='pt')\n",
    "                \n",
    "                return inputs['pixel_values'][:,0]\n",
    "            else:\n",
    "                return mask[:,0]\n",
    "    return mask_transform\n",
    "\n",
    "\n",
    "def get_chained_attr(obj, attr_chain):\n",
    "    attrs = attr_chain.split(\".\")\n",
    "    for attr in attrs:\n",
    "        obj = getattr(obj, attr)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def compress_single_masks(masks, masks_weights, min_size):\n",
    "    # num_masks, seq_len = masks.shape\n",
    "    masks_bool = (masks > 0).int()\n",
    "    sorted_weights, sorted_indices = torch.sort(masks_weights, descending=True)\n",
    "    sorted_indices = sorted_indices[sorted_weights > 0]\n",
    "\n",
    "    \n",
    "    try:\n",
    "        masks_bool = masks_bool[sorted_indices]  # sorted masks\n",
    "    except:\n",
    "        import pdb; pdb.set_trace()\n",
    "        masks_bool = masks_bool[sorted_indices]  # sorted masks\n",
    "    \n",
    "    masks = torch.zeros(*masks_bool.shape[1:]).to(masks.device)\n",
    "    count = 1\n",
    "    for mask in masks_bool:\n",
    "        new_mask = mask.bool() ^ (mask.bool() & masks.bool())\n",
    "        if torch.sum(new_mask) >= min_size:\n",
    "            masks[new_mask] = count\n",
    "            count += 1\n",
    "\n",
    "    masks = masks - 1\n",
    "    masks = masks.int()\n",
    "    masks[masks == -1] = torch.max(masks) + 1\n",
    "\n",
    "    return masks\n",
    "\n",
    "def compress_masks(masks, masks_weights, min_size=0):\n",
    "    new_masks = []\n",
    "    for i in range(len(masks)):\n",
    "        compressed_mask = compress_single_masks(masks[i], masks_weights[i], \n",
    "                                                min_size)\n",
    "        new_masks.append(compressed_mask)\n",
    "    return torch.stack(new_masks)\n",
    "\n",
    "\n",
    "def compress_masks_image(masks, masks_weights, min_size=0):\n",
    "    assert len(masks.shape) == 4 # bsz, num_masks, img_dim_1, img_dim_2 = masks.shape\n",
    "    return compress_masks(masks, masks_weights, min_size)\n",
    "    \n",
    "\n",
    "def compress_masks_text(masks, masks_weights, min_size=0):\n",
    "    assert len(masks.shape) == 3 # bsz, num_masks, seq_len = masks.shape\n",
    "    return compress_masks(masks, masks_weights, min_size)\n",
    "           \n",
    "\n",
    "def _get_inverse_sqrt_with_separate_heads_schedule_with_warmup_lr_lambda(\n",
    "    current_step: int, *, num_warmup_steps: int, \n",
    "    num_steps_per_epoch: int,\n",
    "    timescale: int = None, \n",
    "    num_heads: int = 1, \n",
    "):\n",
    "    epoch = current_step // (num_steps_per_epoch * num_heads)\n",
    "    steps_within_epoch = current_step % num_steps_per_epoch\n",
    "    step_for_curr_head = epoch * num_steps_per_epoch + steps_within_epoch\n",
    "    if step_for_curr_head < num_warmup_steps:\n",
    "        return float(step_for_curr_head) / float(max(1, num_warmup_steps))\n",
    "    shift = timescale - num_warmup_steps\n",
    "    decay = 1.0 / math.sqrt((step_for_curr_head + shift) / timescale)\n",
    "    return decay\n",
    "\n",
    "def get_inverse_sqrt_with_separate_heads_schedule_with_warmup(\n",
    "    optimizer: Optimizer, num_warmup_steps: int, num_steps_per_epoch: int,\n",
    "    timescale: int = None, \n",
    "    num_heads: int = 1, last_epoch: int = -1\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n",
    "    initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases\n",
    "    linearly between 0 and the initial lr set in the optimizer.\n",
    "\n",
    "    Args:\n",
    "        optimizer ([`~torch.optim.Optimizer`]):\n",
    "            The optimizer for which to schedule the learning rate.\n",
    "        num_warmup_steps (`int`):\n",
    "            The number of steps for the warmup phase.\n",
    "        num_training_steps (`int`):\n",
    "            The total number of training steps.\n",
    "        num_cycles (`int`, *optional*, defaults to 1):\n",
    "            The number of hard restarts to use.\n",
    "        last_epoch (`int`, *optional*, defaults to -1):\n",
    "            The index of the last epoch when resuming training.\n",
    "\n",
    "    Return:\n",
    "        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    if timescale is None:\n",
    "        timescale = num_warmup_steps\n",
    "\n",
    "    lr_lambda = partial(\n",
    "        _get_inverse_sqrt_with_separate_heads_schedule_with_warmup_lr_lambda,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_steps_per_epoch=num_steps_per_epoch,\n",
    "        timescale=timescale,\n",
    "        num_heads=num_heads,\n",
    "    )\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "\"\"\"Sparsemax activation function.\n",
    "\n",
    "Pytorch implementation of Sparsemax function from:\n",
    "-- \"From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification\"\n",
    "-- André F. T. Martins, Ramón Fernandez Astudillo (http://arxiv.org/abs/1602.02068)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Sparsemax(nn.Module):\n",
    "    \"\"\"Sparsemax function.\"\"\"\n",
    "\n",
    "    def __init__(self, dim=None):\n",
    "        \"\"\"Initialize sparsemax activation\n",
    "        \n",
    "        Args:\n",
    "            dim (int, optional): The dimension over which to apply the sparsemax function.\n",
    "        \"\"\"\n",
    "        super(Sparsemax, self).__init__()\n",
    "\n",
    "        self.dim = -1 if dim is None else dim\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward function.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor. First dimension should be the batch size\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [batch_size x number_of_logits] Output tensor\n",
    "\n",
    "        \"\"\"\n",
    "        # Sparsemax currently only handles 2-dim tensors,\n",
    "        # so we reshape to a convenient shape and reshape back after sparsemax\n",
    "        device = inputs.device\n",
    "        inputs = inputs.transpose(0, self.dim)\n",
    "        original_size = inputs.size()\n",
    "        inputs = inputs.reshape(inputs.size(0), -1)\n",
    "        inputs = inputs.transpose(0, 1)\n",
    "        dim = 1\n",
    "\n",
    "        number_of_logits = inputs.size(dim)\n",
    "\n",
    "        # Translate input by max for numerical stability\n",
    "        inputs = inputs - torch.max(inputs, dim=dim, keepdim=True)[0].expand_as(inputs)\n",
    "\n",
    "        # Sort input in descending order.\n",
    "        # (NOTE: Can be replaced with linear time selection method described here:\n",
    "        # http://stanford.edu/~jduchi/projects/DuchiShSiCh08.html)\n",
    "        zs = torch.sort(input=inputs, dim=dim, descending=True)[0]\n",
    "        range_tensor = torch.arange(start=1, end=number_of_logits + 1, step=1, \n",
    "                                    device=device, dtype=inputs.dtype).view(1, -1)\n",
    "        range_tensor = range_tensor.expand_as(zs)\n",
    "\n",
    "        # Determine sparsity of projection\n",
    "        bound = 1 + range_tensor * zs\n",
    "        cumulative_sum_zs = torch.cumsum(zs, dim)\n",
    "        is_gt = torch.gt(bound, cumulative_sum_zs).type(inputs.type())\n",
    "        k = torch.max(is_gt * range_tensor, dim, keepdim=True)[0]\n",
    "\n",
    "        # Compute threshold function\n",
    "        zs_sparse = is_gt * zs\n",
    "\n",
    "        # Compute taus\n",
    "        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k\n",
    "        taus = taus.expand_as(inputs)\n",
    "\n",
    "        # Sparsemax\n",
    "        self.output = torch.max(torch.zeros_like(inputs), inputs - taus)\n",
    "\n",
    "        # Reshape back to original shape\n",
    "        output = self.output\n",
    "        output = output.transpose(0, 1)\n",
    "        output = output.reshape(original_size)\n",
    "        output = output.transpose(0, self.dim)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Backward function.\"\"\"\n",
    "        dim = 1\n",
    "\n",
    "        nonzeros = torch.ne(self.output, 0)\n",
    "        sum = torch.sum(grad_output * nonzeros, dim=dim) / torch.sum(nonzeros, dim=dim)\n",
    "        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n",
    "\n",
    "        return self.grad_input\n",
    "    \n",
    "\n",
    "class GroupGenerateLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "       \n",
    "        self.multihead_attns = nn.ModuleList([nn.MultiheadAttention(hidden_dim, \n",
    "                                                                   1, \n",
    "                                                                   batch_first=True) \\\n",
    "                                                for _ in range(num_heads)])\n",
    "        self.sparsemax = Sparsemax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key_value, epoch=0):\n",
    "        \"\"\"\n",
    "            Use multiheaded attention to get mask\n",
    "            Num_interpretable_heads = num_heads * seq_len\n",
    "            Input: x (bsz, seq_len, hidden_dim)\n",
    "                   if actual_x is not None, then use actual_x instead of x to compute attn_output\n",
    "            Output: attn_outputs (bsz, num_heads * seq_len, seq_len, hidden_dim)\n",
    "                    mask (bsz, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        epsilon = 1e-30\n",
    "\n",
    "        if epoch == -1:\n",
    "            epoch = self.num_heads\n",
    "        \n",
    "        head_i = epoch % self.num_heads\n",
    "        if self.training:\n",
    "            _, attn_weights = self.multihead_attns[head_i](query, key_value, key_value, \n",
    "                                                          average_attn_weights=False)\n",
    "        else:\n",
    "            attn_weights = []\n",
    "            if epoch < self.num_heads:\n",
    "                num_heads_use = head_i + 1\n",
    "            else:\n",
    "                num_heads_use = self.num_heads\n",
    "            for head_j in range(num_heads_use):\n",
    "                _, attn_weights_j = self.multihead_attns[head_j](query, key_value, key_value)\n",
    "                attn_weights.append(attn_weights_j)\n",
    "            attn_weights = torch.stack(attn_weights, dim=1)\n",
    "        \n",
    "        attn_weights = attn_weights + epsilon\n",
    "        mask = self.sparsemax(torch.log(attn_weights))\n",
    "            \n",
    "        return mask\n",
    "\n",
    "\n",
    "class GroupSelectLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_dim, 1, \n",
    "                                                    batch_first=True)\n",
    "        self.sparsemax = Sparsemax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        \"\"\"\n",
    "            Use multiheaded attention to get mask\n",
    "            Num_heads = num_heads * seq_len\n",
    "            Input: x (bsz, seq_len, hidden_dim)\n",
    "                   if actual_x is not None, then use actual_x instead of x to compute attn_output\n",
    "            Output: attn_outputs (bsz, num_heads * seq_len, seq_len, hidden_dim)\n",
    "                    mask (bsz, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # x shape: (batch_size, sequence_length, hidden_dim)\n",
    "        # x shape: (..., hidden_dim)\n",
    "        epsilon = 1e-30\n",
    "        bsz, seq_len, hidden_dim = query.shape\n",
    "\n",
    "        # Obtain attention weights\n",
    "        _, attn_weights = self.multihead_attn(query, key, key)\n",
    "        attn_weights = attn_weights + epsilon  # (batch_size, num_heads, sequence_length, hidden_dim)\n",
    "        mask = self.sparsemax(torch.log(attn_weights))\n",
    "        mask = mask.transpose(-1, -2)\n",
    "\n",
    "        # Apply attention weights on what to be attended\n",
    "        new_shape = list(mask.shape) + [1] * (len(value.shape) - 3)\n",
    "        attn_outputs = (value * mask.view(*new_shape)).sum(1)\n",
    "\n",
    "        # attn_outputs of shape (bsz, num_masks, num_classes)\n",
    "        return attn_outputs, mask\n",
    "\n",
    "\n",
    "class SOPConfig(PretrainedConfig):\n",
    "    def __init__(self,\n",
    "                 json_file=None,\n",
    "                 hidden_size: int = None,\n",
    "                 num_labels: int = None,\n",
    "                 projected_input_scale: int = None,\n",
    "                 num_heads: int = None,\n",
    "                 num_masks_sample: int = None,\n",
    "                 num_masks_max: int = None,\n",
    "                 image_size=None,\n",
    "                 num_channels: int = None,\n",
    "                 attn_patch_size: int = None,\n",
    "                 finetune_layers=None,\n",
    "                ):\n",
    "        # all the config from the json file will be in self.__dict__\n",
    "        super().__init__()\n",
    "\n",
    "        # set default values\n",
    "        self.hidden_size = 512\n",
    "        self.num_labels = 2\n",
    "        self.projected_input_scale = 1\n",
    "        self.num_heads = 1\n",
    "        self.num_masks_sample = 20\n",
    "        self.num_masks_max = 200\n",
    "        self.image_size=(224, 224)\n",
    "        self.num_channels = 3\n",
    "        self.attn_patch_size = 16\n",
    "        self.finetune_layers=[]\n",
    "\n",
    "        # first load the config from json file if specified\n",
    "        if json_file is not None:\n",
    "            self.update_from_json(json_file)\n",
    "        \n",
    "        # overwrite the config from json file if specified\n",
    "        if hidden_size is not None:\n",
    "            self.hidden_size = hidden_size\n",
    "        if num_labels is not None: \n",
    "            self.num_labels = num_labels\n",
    "        if projected_input_scale is not None:\n",
    "            self.projected_input_scale = projected_input_scale\n",
    "        if num_heads is not None:\n",
    "            self.num_heads = num_heads\n",
    "        if num_masks_sample is not None:\n",
    "            self.num_masks_sample = num_masks_sample\n",
    "        if num_masks_max is not None:\n",
    "            self.num_masks_max = num_masks_max\n",
    "        if image_size is not None:\n",
    "            self.image_size = image_size\n",
    "        if num_channels is not None:\n",
    "            self.num_channels = num_channels\n",
    "        if attn_patch_size is not None:\n",
    "            self.attn_patch_size = attn_patch_size\n",
    "        if finetune_layers is not None:\n",
    "            self.finetune_layers = finetune_layers\n",
    "\n",
    "        \n",
    "    def update_from_json(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            json_dict = json.load(f)\n",
    "        self.__dict__.update(json_dict)\n",
    "\n",
    "    def save_to_json(self, json_file):\n",
    "        attrs_save = [\n",
    "            'hidden_size',\n",
    "            'num_labels',\n",
    "            'projected_input_scale',\n",
    "            'num_heads',\n",
    "            'num_masks_sample',\n",
    "            'num_masks_max',\n",
    "            'image_size',\n",
    "            'num_channels',\n",
    "            'attn_patch_size',\n",
    "            'finetune_layers'\n",
    "        ]\n",
    "        to_save = {k: v for k, v in self.__dict__.items() if k in attrs_save}\n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(to_save, f, indent=4)\n",
    "\n",
    "\n",
    "class SOP(PreTrainedModel):\n",
    "    config_class = SOPConfig\n",
    "\n",
    "    def __init__(self, \n",
    "                 config,\n",
    "                 backbone_model,\n",
    "                 class_weights=None,\n",
    "                 ):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size  # match black_box_model hidden_size\n",
    "        self.num_classes = config.num_labels if hasattr(config, 'num_labels') is not None else 1  # 1 is for regression\n",
    "        self.projected_input_scale = config.projected_input_scale if hasattr(config, 'projected_input_scale') else 1\n",
    "        self.num_heads = config.num_heads\n",
    "        self.num_masks_sample = config.num_masks_sample\n",
    "        self.num_masks_max = config.num_masks_max\n",
    "        self.finetune_layers = config.finetune_layers\n",
    "\n",
    "        # blackbox model and finetune layers\n",
    "        self.blackbox_model = backbone_model\n",
    "        if class_weights is None:\n",
    "            try:\n",
    "                class_weights = get_chained_attr(backbone_model, config.finetune_layers[0]).weight\n",
    "            except:\n",
    "                raise ValueError('class_weights is None and cannot be inferred from backbone_model')\n",
    "        try:\n",
    "            self.class_weights = copy.deepcopy(class_weights)  # maybe can do this outside\n",
    "            print('deep copy class weights')\n",
    "        except:\n",
    "            self.class_weights = class_weights.clone()\n",
    "            print('shallow copy class weights')\n",
    "        \n",
    "        \n",
    "        self.input_attn = GroupGenerateLayer(hidden_dim=self.hidden_size,\n",
    "                                             num_heads=self.num_heads)\n",
    "        # output\n",
    "        self.output_attn = GroupSelectLayer(hidden_dim=self.hidden_size)\n",
    "\n",
    "    def init_grads(self):\n",
    "        # Initialize the weights of the model\n",
    "        # blackbox_model = self.blackbox_model.clone()\n",
    "        # self.init_weights()\n",
    "        # self.blackbox_model = blackbox_model\n",
    "\n",
    "        for name, param in self.blackbox_model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for finetune_layer in self.finetune_layers:\n",
    "            # todo: check if this works with index\n",
    "            for name, param in get_chained_attr(self.blackbox_model, finetune_layer).named_parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def save(self, save_dir):\n",
    "        self.config.save_to_json(os.path.join(save_dir, 'config.json'))\n",
    "        torch.save(self.state_dict(), os.path.join(save_dir, 'model.pt'))\n",
    "        print('Saved model to {}'.format(save_dir))\n",
    "\n",
    "    def load(self, save_dir):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.config.update_from_json(os.path.join(save_dir, 'config.json'))\n",
    "        self.load_state_dict(torch.load(os.path.join(save_dir, 'model.pt'), map_location=device))\n",
    "        print('Loaded model from {}'.format(save_dir))\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        self.load_state_dict(torch.load(checkpoint_path)['model'])\n",
    "        print('Loaded model from checkpoint {}'.format(checkpoint_path))\n",
    "\n",
    "\n",
    "class SOPImage(SOP):\n",
    "    def __init__(self, \n",
    "                 config,\n",
    "                 blackbox_model,\n",
    "                 class_weights=None,\n",
    "                 projection_layer=None,\n",
    "                 ):\n",
    "        super().__init__(config,\n",
    "                            blackbox_model,\n",
    "                            class_weights\n",
    "                            )\n",
    "        \n",
    "        self.image_size = config.image_size if isinstance(config.image_size, \n",
    "                                                    collections.abc.Iterable) \\\n",
    "                                            else (config.image_size, config.image_size)\n",
    "        self.num_channels = config.num_channels\n",
    "        # attention args\n",
    "        self.attn_patch_size = config.attn_patch_size\n",
    "\n",
    "        if projection_layer is not None:\n",
    "            self.projection = copy.deepcopy(projection_layer)\n",
    "        else:\n",
    "            self.init_projection()\n",
    "\n",
    "        # Initialize the weights of the model\n",
    "        # Store initial weights for comparison\n",
    "        self.initial_weights = self.get_weights()\n",
    "\n",
    "        # Initialize the weights of the model\n",
    "        self.init_weights()\n",
    "\n",
    "        # Check if weights have changed\n",
    "        self.check_weights_changed()\n",
    "\n",
    "        self.init_grads()\n",
    "        \n",
    "    \n",
    "    def get_weights(self):\n",
    "        # Get the weights of the model\n",
    "        weights = {}\n",
    "        for name, param in self.named_parameters():\n",
    "            weights[name] = param.data.clone()\n",
    "        return weights\n",
    "\n",
    "    def check_weights_changed(self):\n",
    "        # Check if weights have changed after initialization\n",
    "        changed = False\n",
    "        for name, initial_param in self.initial_weights.items():\n",
    "            current_param = self.state_dict()[name]\n",
    "            if not torch.equal(current_param, initial_param):\n",
    "                print(f\"Weights changed for layer: {name}\")\n",
    "                changed = True\n",
    "        if not changed:\n",
    "            print(\"No weights have changed after initialization.\")\n",
    "    def init_projection(self):\n",
    "        self.projection = nn.Conv2d(self.config.num_channels, \n",
    "                                    self.hidden_size, \n",
    "                                    kernel_size=self.attn_patch_size, \n",
    "                                    stride=self.attn_patch_size)  # make each patch a vec\n",
    "        self.projection_up = nn.ConvTranspose2d(1, \n",
    "                                                    1, \n",
    "                                                    kernel_size=self.attn_patch_size, \n",
    "                                                    stride=self.attn_patch_size)  # make each patch a vec\n",
    "        self.projection_up.weight = nn.Parameter(torch.ones_like(self.projection_up.weight))\n",
    "        self.projection_up.bias = nn.Parameter(torch.zeros_like(self.projection_up.bias))\n",
    "        self.projection_up.weight.requires_grad = False\n",
    "        self.projection_up.bias.requires_grad = False\n",
    "\n",
    "    def forward(self, \n",
    "                inputs, \n",
    "                segs=None, \n",
    "                input_mask_weights=None,\n",
    "                epoch=-1, \n",
    "                mask_batch_size=16,\n",
    "                label=None,\n",
    "                return_tuple=False):\n",
    "        if epoch == -1:\n",
    "            epoch = self.num_heads\n",
    "        bsz, num_channel, img_dim1, img_dim2 = inputs.shape\n",
    "        \n",
    "        # Mask (Group) generation\n",
    "        if input_mask_weights is None:\n",
    "            grouped_inputs, input_mask_weights = self.group_generate(inputs, epoch, mask_batch_size, segs)\n",
    "        else:\n",
    "            grouped_inputs = inputs.unsqueeze(1) * input_mask_weights.unsqueeze(2) # directly apply mask\n",
    "        \n",
    "        # Backbone model\n",
    "        logits, pooler_outputs = self.run_backbone(grouped_inputs, mask_batch_size)\n",
    "        # return logits\n",
    "\n",
    "        # Mask (Group) selection & aggregation\n",
    "        # return self.group_select(logits, pooler_outputs, img_dim1, img_dim2)\n",
    "        weighted_logits, output_mask_weights, logits, pooler_outputs = self.group_select(logits, pooler_outputs, img_dim1, img_dim2)\n",
    "\n",
    "        if return_tuple:\n",
    "            return self.get_results_tuple(weighted_logits, logits, pooler_outputs, input_mask_weights, output_mask_weights, bsz, label)\n",
    "        else:\n",
    "            return weighted_logits\n",
    "\n",
    "    def get_results_tuple(self, weighted_logits, logits, pooler_outputs, input_mask_weights, output_mask_weights, bsz, label):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run_backbone(self, masked_inputs, mask_batch_size):\n",
    "        bsz, num_masks, num_channel, img_dim1, img_dim2 = masked_inputs.shape\n",
    "        masked_inputs = masked_inputs.view(-1, num_channel, img_dim1, img_dim2)\n",
    "        logits = []\n",
    "        pooler_outputs = []\n",
    "        for i in range(0, masked_inputs.shape[0], mask_batch_size):\n",
    "            output_i = self.blackbox_model(\n",
    "                masked_inputs[i:i+mask_batch_size]\n",
    "            )\n",
    "            pooler_i = output_i.pooler_output\n",
    "            logits_i = output_i.logits\n",
    "            logits.append(logits_i)\n",
    "            pooler_outputs.append(pooler_i)\n",
    "\n",
    "        logits = torch.cat(logits).view(bsz, num_masks, self.num_classes, -1)\n",
    "        pooler_outputs = torch.cat(pooler_outputs).view(bsz, num_masks, self.hidden_size, -1)\n",
    "        return logits, pooler_outputs\n",
    "    \n",
    "    def group_generate(self, inputs, epoch, mask_batch_size, segs=None):\n",
    "        bsz, num_channel, img_dim1, img_dim2 = inputs.shape\n",
    "        if segs is None:   # should be renamed \"segments\"\n",
    "            projected_inputs = self.projection(inputs)\n",
    "            projected_inputs = projected_inputs.flatten(2).transpose(1, 2)  # bsz, img_dim1 * img_dim2, num_channel\n",
    "            projected_inputs = projected_inputs * self.projected_input_scale\n",
    "\n",
    "            if self.num_masks_max != -1:\n",
    "                input_dropout_idxs = torch.randperm(projected_inputs.shape[1])[:self.num_masks_max]\n",
    "                projected_query = projected_inputs[:, input_dropout_idxs]\n",
    "            else:\n",
    "                projected_query = projected_inputs\n",
    "            input_mask_weights_cand = self.input_attn(projected_query, projected_inputs, epoch=epoch)\n",
    "            \n",
    "            num_patches = ((self.image_size[0] - self.attn_patch_size) // self.attn_patch_size + 1, \n",
    "                        (self.image_size[1] - self.attn_patch_size) // self.attn_patch_size + 1)\n",
    "            input_mask_weights_cand = input_mask_weights_cand.reshape(-1, 1, num_patches[0], num_patches[1])\n",
    "            input_mask_weights_cand = self.projection_up(input_mask_weights_cand, \n",
    "                                                         output_size=torch.Size([input_mask_weights_cand.shape[0], 1, \n",
    "                                                                                 img_dim1, img_dim2]))\n",
    "            input_mask_weights_cand = input_mask_weights_cand.view(bsz, -1, img_dim1, img_dim2)\n",
    "            input_mask_weights_cand = torch.clip(input_mask_weights_cand, max=1.0)\n",
    "        else:\n",
    "            # With/without masks are a bit different. Should we make them the same? Need to experiment.\n",
    "            bsz, num_segs, img_dim1, img_dim2 = segs.shape\n",
    "            seged_output_0 = inputs.unsqueeze(1) * segs.unsqueeze(2) # (bsz, num_masks, num_channel, img_dim1, img_dim2)\n",
    "            # import pdb; pdb.set_trace()\n",
    "            _, interm_outputs = self.run_backbone(seged_output_0, mask_batch_size)\n",
    "            \n",
    "            interm_outputs = interm_outputs.view(bsz, -1, self.hidden_size)\n",
    "            interm_outputs = interm_outputs * self.projected_input_scale\n",
    "            segment_mask_weights = self.input_attn(interm_outputs, interm_outputs, epoch=epoch)\n",
    "            segment_mask_weights = segment_mask_weights.reshape(bsz, -1, num_segs)\n",
    "            \n",
    "            new_masks =  segs.unsqueeze(1) * segment_mask_weights.unsqueeze(-1).unsqueeze(-1)\n",
    "            # (bsz, num_new_masks, num_masks, img_dim1, img_dim2)\n",
    "            input_mask_weights_cand = new_masks.sum(2)  # if one mask has it, then have it\n",
    "            # todo: Can we simplify the above to be dot product?\n",
    "            \n",
    "        scale_factor = 1.0 / input_mask_weights_cand.reshape(bsz, -1, \n",
    "                                                        img_dim1 * img_dim2).max(dim=-1).values\n",
    "        input_mask_weights_cand = input_mask_weights_cand * scale_factor.view(bsz, -1,1,1)\n",
    "        \n",
    "        \n",
    "        # we are using iterative training\n",
    "        # we will train some masks every epoch\n",
    "        # the masks to train are selected by mod of epoch number\n",
    "        # Dropout for training\n",
    "        if self.training:\n",
    "            dropout_idxs = torch.randperm(input_mask_weights_cand.shape[1])[:self.num_masks_sample]\n",
    "            dropout_mask = torch.zeros(bsz, input_mask_weights_cand.shape[1]).to(inputs.device)\n",
    "            dropout_mask[:,dropout_idxs] = 1\n",
    "        else:\n",
    "            dropout_mask = torch.ones(bsz, input_mask_weights_cand.shape[1]).to(inputs.device)\n",
    "        \n",
    "        input_mask_weights = input_mask_weights_cand[dropout_mask.bool()].clone()\n",
    "        input_mask_weights = input_mask_weights.view(bsz, -1, img_dim1, img_dim2)\n",
    "\n",
    "        masked_inputs = inputs.unsqueeze(1) * input_mask_weights.unsqueeze(2)\n",
    "        return masked_inputs, input_mask_weights\n",
    "    \n",
    "    def group_select(self, logits, pooler_outputs, img_dim1, img_dim2):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SOPImageCls(SOPImage):\n",
    "    def group_select(self, logits, pooler_outputs, img_dim1, img_dim2):\n",
    "        bsz, num_masks = logits.shape[:2]\n",
    "\n",
    "        logits = logits.view(bsz, num_masks, self.num_classes)\n",
    "        pooler_outputs = pooler_outputs.view(bsz, num_masks, self.hidden_size)\n",
    "\n",
    "        query = self.class_weights.unsqueeze(0).expand(bsz, \n",
    "                                                    self.num_classes, \n",
    "                                                    self.hidden_size) #.to(logits.device)\n",
    "        \n",
    "        key = pooler_outputs\n",
    "        weighted_logits, output_mask_weights = self.output_attn(query, key, logits)\n",
    "\n",
    "        return weighted_logits, output_mask_weights, logits, pooler_outputs\n",
    "    \n",
    "    def get_results_tuple(self, weighted_logits, logits, pooler_outputs, input_mask_weights, output_mask_weights, bsz, label):\n",
    "        # todo: debug for segmentation\n",
    "        masks_aggr = None\n",
    "        masks_aggr_pred_cls = None\n",
    "        masks_max_pred_cls = None\n",
    "        flat_masks = None\n",
    "\n",
    "        if label is not None:\n",
    "            predicted = label  # allow labels to be different\n",
    "        else:\n",
    "            _, predicted = torch.max(weighted_logits.data, -1)\n",
    "        \n",
    "        grouped_attributions = output_mask_weights * logits # instead of just output_mask_weights\n",
    "        # import pdb; pdb.set_trace()\n",
    "        masks_mult_pred = input_mask_weights * grouped_attributions[range(len(predicted)),:,predicted,None,None]\n",
    "        masks_aggr_pred_cls = masks_mult_pred.sum(1)[:,None,:,:]\n",
    "        max_mask_indices = grouped_attributions.max(2).values.max(1).indices\n",
    "        # import pdb; pdb.set_trace()\n",
    "        masks_max_pred_cls = masks_mult_pred[range(bsz),max_mask_indices]\n",
    "\n",
    "        flat_masks = compress_masks_image(input_mask_weights, grouped_attributions[:,:,predicted])\n",
    "        \n",
    "        return AttributionOutputSOP(weighted_logits,\n",
    "                                    logits,\n",
    "                                    pooler_outputs,\n",
    "                                    input_mask_weights,\n",
    "                                    output_mask_weights,\n",
    "                                    masks_aggr_pred_cls,\n",
    "                                    masks_max_pred_cls,\n",
    "                                    masks_aggr,\n",
    "                                    flat_masks,\n",
    "                                    grouped_attributions)\n",
    "    \n",
    "\n",
    "class SOPImageSeg(SOPImage):\n",
    "    def group_select(self, logits, pooler_outputs, img_dim1, img_dim2):\n",
    "        bsz, num_masks = logits.shape[:2]\n",
    "        logits = logits.view(bsz, num_masks, self.num_classes, \n",
    "                                            img_dim1, img_dim2)\n",
    "        pooler_outputs = pooler_outputs.view(bsz, num_masks, \n",
    "                                                        self.hidden_size, \n",
    "                                                        img_dim1, \n",
    "                                                        img_dim2)\n",
    "        # return pooler_outputs\n",
    "        query = self.class_weights.unsqueeze(0) \\\n",
    "            .view(1, self.num_classes, self.hidden_size, -1).mean(-1) \\\n",
    "            .expand(bsz, self.num_classes, self.hidden_size).to(logits.device)\n",
    "        pooler_outputs.requires_grad = True\n",
    "        key = pooler_outputs.view(bsz, num_masks, self.hidden_size, -1).mean(-1)\n",
    "        weighted_logits, output_mask_weights = self.output_attn(query, key, logits)\n",
    "\n",
    "        return weighted_logits, output_mask_weights, logits, pooler_outputs\n",
    "    \n",
    "    def get_results_tuple(self, weighted_logits, logits, pooler_outputs, input_mask_weights, output_mask_weights, bsz, label):\n",
    "        # todo: debug for segmentation\n",
    "        masks_aggr = None\n",
    "        masks_aggr_pred_cls = None\n",
    "        masks_max_pred_cls = None\n",
    "        flat_masks = None\n",
    "\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        _, predicted = torch.max(weighted_logits.data, -1)\n",
    "        masks_mult = input_mask_weights.unsqueeze(2) * output_mask_weights.unsqueeze(-1).unsqueeze(-1) # bsz, n_masks, n_cls, img_dim, img_dim\n",
    "        masks_aggr = masks_mult.sum(1) # bsz, n_cls, img_dim, img_dim OR bsz, n_cls, seq_len\n",
    "        # masks_aggr_pred_cls = masks_aggr\n",
    "        # masks_aggr_pred_cls = masks_aggr[range(bsz), predicted].unsqueeze(1)\n",
    "        # max_mask_indices = output_mask_weights.argmax(1)\n",
    "        # masks_max_pred_cls = masks_mult[max_mask_indices[:,0]]\n",
    "        # masks_max_pred_cls = max_mask_indices\n",
    "        # TODO: this has some problems ^\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        grouped_attributions = output_mask_weights * logits\n",
    "        \n",
    "        flat_masks = compress_masks_image(input_mask_weights, output_mask_weights[:,:,predicted])\n",
    "\n",
    "        return AttributionOutputSOP(weighted_logits,\n",
    "                                    logits,\n",
    "                                    pooler_outputs,\n",
    "                                    input_mask_weights,\n",
    "                                    output_mask_weights,\n",
    "                                    masks_aggr_pred_cls,\n",
    "                                    masks_max_pred_cls,\n",
    "                                    masks_aggr,\n",
    "                                    flat_masks,\n",
    "                                    grouped_attributions)\n",
    "\n",
    "\n",
    "class SOPText(SOP):\n",
    "    def __init__(self, \n",
    "                 config,\n",
    "                 blackbox_model,\n",
    "                 class_weights=None,\n",
    "                 projection_layer=None,\n",
    "                 ):\n",
    "        super().__init__(config,\n",
    "                            blackbox_model,\n",
    "                            class_weights\n",
    "                            )\n",
    "\n",
    "        if projection_layer is not None:\n",
    "            self.projection = copy.deepcopy(projection_layer)\n",
    "        else:\n",
    "            self.init_projection()\n",
    "\n",
    "        # Initialize the weights of the model\n",
    "        self.init_grads()\n",
    "\n",
    "    def init_projection(self):\n",
    "        self.projection = nn.Linear(1, self.hidden_size)\n",
    "\n",
    "    def forward(self, \n",
    "                inputs, \n",
    "                segs=None, \n",
    "                input_mask_weights=None,\n",
    "                epoch=-1, \n",
    "                mask_batch_size=16,\n",
    "                label=None,\n",
    "                return_tuple=False,\n",
    "                kwargs={}):\n",
    "        if epoch == -1:\n",
    "            epoch = self.num_heads\n",
    "        bsz, seq_len = inputs.shape\n",
    "        \n",
    "        # Mask (Group) generation\n",
    "        if input_mask_weights is None:\n",
    "            grouped_inputs_embeds, input_mask_weights, grouped_kwargs = self.group_generate(inputs, epoch, mask_batch_size, \n",
    "                                                                                            segs, kwargs)\n",
    "            grouped_inputs = None\n",
    "        else:\n",
    "            grouped_inputs = inputs.unsqueeze(1) * input_mask_weights.unsqueeze(2) # directly apply mask\n",
    "        \n",
    "        # Backbone model\n",
    "        logits, pooler_outputs = self.run_backbone(grouped_inputs, mask_batch_size, kwargs=grouped_kwargs)\n",
    "\n",
    "        # Mask (Group) selection & aggregation\n",
    "        weighted_logits, output_mask_weights, logits, pooler_outputs = self.group_select(logits, pooler_outputs, seq_len)\n",
    "\n",
    "        if return_tuple:\n",
    "            return self.get_results_tuple(weighted_logits, logits, pooler_outputs, input_mask_weights, output_mask_weights, bsz, label)\n",
    "        else:\n",
    "            return weighted_logits\n",
    "\n",
    "    def get_results_tuple(self, weighted_logits, logits, pooler_outputs, input_mask_weights, output_mask_weights, bsz, label):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run_backbone(self, masked_inputs=None, mask_batch_size=16, kwargs={}):  # TODO: Fix so that we don't need to know the input\n",
    "        if masked_inputs is not None:\n",
    "            bsz, num_masks, seq_len = masked_inputs.shape\n",
    "            masked_inputs = masked_inputs.reshape(-1, seq_len)\n",
    "            kwargs_flat = {k: v.reshape(-1, seq_len) for k, v in kwargs.items()}\n",
    "        else:\n",
    "            bsz, num_masks, seq_len, hidden_size = kwargs['inputs_embeds'].shape\n",
    "            \n",
    "            kwargs_flat = {k: v.reshape(-1, seq_len, hidden_size) if k == 'inputs_embeds' else v.reshape(-1, seq_len)\n",
    "                           for k, v in kwargs.items()}\n",
    "        logits = []\n",
    "        pooler_outputs = []\n",
    "        for i in range(0, bsz * num_masks, mask_batch_size):\n",
    "            kwargs_i = {k: v[i:i+mask_batch_size] for k, v in kwargs_flat.items()}\n",
    "            output_i = self.blackbox_model(\n",
    "                masked_inputs[i:i+mask_batch_size] if masked_inputs is not None else None,\n",
    "                **kwargs_i\n",
    "            )\n",
    "            pooler_i = output_i.pooler_output\n",
    "            logits_i = output_i.logits\n",
    "            logits.append(logits_i)\n",
    "            pooler_outputs.append(pooler_i)\n",
    "\n",
    "        logits = torch.cat(logits).view(bsz, num_masks, self.num_classes, -1)\n",
    "        pooler_outputs = torch.cat(pooler_outputs).view(bsz, num_masks, self.hidden_size, -1)\n",
    "        return logits, pooler_outputs\n",
    "    \n",
    "    def group_generate(self, inputs, epoch, mask_batch_size=16, segs=None, kwargs={}):\n",
    "        bsz, seq_len = inputs.shape\n",
    "        mask_embed = self.projection(torch.tensor([0]).int().to(inputs.device))\n",
    "        projected_inputs = self.projection(inputs)\n",
    "        \n",
    "        if segs is None:   # word level\n",
    "            projected_inputs = projected_inputs * self.projected_input_scale\n",
    "\n",
    "            if self.num_masks_max != -1:\n",
    "                input_dropout_idxs = torch.randperm(projected_inputs.shape[1]).to(inputs.device)\n",
    "                if 'attention_mask' in kwargs:\n",
    "                    attention_mask_mult = kwargs['attention_mask'] * input_dropout_idxs\n",
    "                else:\n",
    "                    attention_mask_mult = input_dropout_idxs\n",
    "                input_dropout_idxs = torch.argsort(attention_mask_mult, dim=-1).flip(-1)[:, :self.num_masks_max]\n",
    "                batch_indices = torch.arange(bsz).unsqueeze(1).repeat(1, input_dropout_idxs.shape[-1])\n",
    "                selected_projected_inputs = projected_inputs[batch_indices, input_dropout_idxs]\n",
    "                projected_query = selected_projected_inputs\n",
    "            else:\n",
    "                projected_query = projected_inputs\n",
    "            input_mask_weights_cand = self.input_attn(projected_query, projected_inputs, epoch=epoch)\n",
    "            input_mask_weights_cand = input_mask_weights_cand.squeeze(1)\n",
    "\n",
    "            input_mask_weights_cand = torch.clip(input_mask_weights_cand, max=1.0)\n",
    "        else: # sentence level\n",
    "            # With/without masks are a bit different. Should we make them the same? Need to experiment.\n",
    "            bsz, num_segs, seq_len = segs.shape\n",
    "\n",
    "            seged_inputs_embeds = projected_inputs.unsqueeze(1) * segs.unsqueeze(-1) + \\\n",
    "                               mask_embed.view(1,1,1,-1) * (1 - segs.unsqueeze(-1))\n",
    "            \n",
    "            seged_kwargs = {}\n",
    "            for k, v in kwargs.items():\n",
    "                seged_kwargs[k] = v.unsqueeze(1).expand(segs.shape).reshape(-1, seq_len)\n",
    "            seged_kwargs['inputs_embeds'] = seged_inputs_embeds\n",
    "\n",
    "            # TODO: always have seg for the part after sep token\n",
    "            _, interm_outputs = self.run_backbone(None, mask_batch_size, kwargs=seged_kwargs)\n",
    "            \n",
    "            interm_outputs = interm_outputs.view(bsz, -1, self.hidden_size)\n",
    "            interm_outputs = interm_outputs * self.projected_input_scale\n",
    "            segment_mask_weights = self.input_attn(interm_outputs, interm_outputs, epoch=epoch)\n",
    "            segment_mask_weights = segment_mask_weights.reshape(bsz, -1, num_segs)\n",
    "            \n",
    "            new_masks =  segs.unsqueeze(1) * segment_mask_weights.unsqueeze(-1)\n",
    "            # (bsz, num_new_masks, num_masks, seq_len)\n",
    "            input_mask_weights_cand = new_masks.sum(2)  # if one mask has it, then have it\n",
    "            # todo: Can we simplify the above to be dot product?\n",
    "            \n",
    "        scale_factor = 1.0 / input_mask_weights_cand.max(dim=-1).values\n",
    "        input_mask_weights_cand = input_mask_weights_cand * scale_factor.view(bsz, -1,1)\n",
    "\n",
    "        # we are using iterative training\n",
    "        # we will train some masks every epoch\n",
    "        # the masks to train are selected by mod of epoch number\n",
    "        # Dropout for training\n",
    "        if self.training:\n",
    "            dropout_idxs = torch.randperm(input_mask_weights_cand.shape[1])[:self.num_masks_sample]\n",
    "            dropout_mask = torch.zeros(bsz, input_mask_weights_cand.shape[1]).to(inputs.device)\n",
    "            dropout_mask[:,dropout_idxs] = 1\n",
    "        else:\n",
    "            dropout_mask = torch.ones(bsz, input_mask_weights_cand.shape[1]).to(inputs.device)\n",
    "        \n",
    "        input_mask_weights = input_mask_weights_cand[dropout_mask.bool()].clone()\n",
    "        input_mask_weights = input_mask_weights.reshape(bsz, -1, seq_len)\n",
    "        \n",
    "        # Always add the second part of the sequence (in question answering, it would be the qa pair)\n",
    "        input_mask_weights = input_mask_weights  + kwargs['token_type_ids'].unsqueeze(1)\n",
    "        \n",
    "        masked_inputs_embeds = projected_inputs.unsqueeze(1) * input_mask_weights.unsqueeze(-1) + \\\n",
    "                               mask_embed.view(1,1,1,-1) * (1 - input_mask_weights.unsqueeze(-1))\n",
    "        \n",
    "        masked_kwargs = {}\n",
    "        for k, v in kwargs.items():\n",
    "            masked_kwargs[k] = v.unsqueeze(1).expand(input_mask_weights.shape).reshape(-1, seq_len)\n",
    "        masked_kwargs['inputs_embeds'] = masked_inputs_embeds\n",
    "        \n",
    "        return masked_inputs_embeds, input_mask_weights, masked_kwargs\n",
    "    \n",
    "    def group_select(self, logits, pooler_outputs, seq_len):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SOPTextCls(SOPText):\n",
    "    def group_select(self, logits, pooler_outputs, seq_len):\n",
    "        bsz, num_masks = logits.shape[:2]\n",
    "\n",
    "        logits = logits.view(bsz, num_masks, self.num_classes)\n",
    "        pooler_outputs = pooler_outputs.view(bsz, num_masks, self.hidden_size)\n",
    "\n",
    "        query = self.class_weights.unsqueeze(0).expand(bsz, \n",
    "                                                    self.num_classes, \n",
    "                                                    self.hidden_size) #.to(logits.device)\n",
    "        \n",
    "        key = pooler_outputs\n",
    "        weighted_logits, output_mask_weights = self.output_attn(query, key, logits)\n",
    "\n",
    "        return weighted_logits, output_mask_weights, logits, pooler_outputs\n",
    "    \n",
    "    def get_results_tuple(self, weighted_logits, logits, pooler_outputs, input_mask_weights, output_mask_weights, bsz, label):\n",
    "        # todo: debug for segmentation\n",
    "        masks_aggr = None\n",
    "        masks_aggr_pred_cls = None\n",
    "        masks_max_pred_cls = None\n",
    "        flat_masks = None\n",
    "\n",
    "        if label is not None:\n",
    "            predicted = label  # allow labels to be different\n",
    "        else:\n",
    "            _, predicted = torch.max(weighted_logits.data, -1)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        # masks_mult = input_mask_weights.unsqueeze(2) * output_mask_weights.unsqueeze(-1) # bsz, n_masks, n_cls\n",
    "        \n",
    "        # masks_aggr = masks_mult.sum(1) # bsz, n_cls\n",
    "        # masks_aggr_pred_cls = masks_aggr[range(bsz), predicted].unsqueeze(1)\n",
    "        # max_mask_indices = output_mask_weights.max(2).values.max(1).indices\n",
    "        # masks_max_pred_cls = masks_mult[range(bsz),max_mask_indices,predicted].unsqueeze(1)\n",
    "\n",
    "        masks_mult_pred = input_mask_weights * output_mask_weights[:,:,predicted]\n",
    "        masks_aggr_pred_cls = masks_mult_pred.sum(1)\n",
    "        max_mask_indices = output_mask_weights.max(2).values.max(1).indices\n",
    "        masks_max_pred_cls = masks_mult_pred[range(bsz),max_mask_indices]\n",
    "\n",
    "        flat_masks = compress_masks_text(input_mask_weights, output_mask_weights[:,:,predicted])\n",
    "        grouped_attributions = output_mask_weights * logits\n",
    "        return AttributionOutputSOP(weighted_logits,\n",
    "                                    logits,\n",
    "                                    pooler_outputs,\n",
    "                                    input_mask_weights,\n",
    "                                    output_mask_weights,\n",
    "                                    masks_aggr_pred_cls,\n",
    "                                    masks_max_pred_cls,\n",
    "                                    masks_aggr,\n",
    "                                    flat_masks,\n",
    "                                    grouped_attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebdbef0b-ef0b-403b-97f2-24c9b67fdaa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "SEED = 42\n",
    "if SEED != -1:\n",
    "    # Torch RNG\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    # Python RNG\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c936663e-2c91-49af-a263-30ffbeeda7d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model paths\n",
    "backbone_model_name = '../../data/cosmogrid/CNN_mass_maps.pth'\n",
    "\n",
    "# data paths\n",
    "TRAIN_DATA_DIR = '../../data/cosmogrid'\n",
    "VAL_DATA_DIR = '../../data/cosmogrid'\n",
    "mask_path = '../../data/processed/cosmogrid/masks/X_maps_Cosmogrid_100k_watershed_diagonal.npy'\n",
    "\n",
    "# training args\n",
    "batch_size = 16\n",
    "lr = 0.0005\n",
    "num_epochs = 20\n",
    "warmup_steps = 2000\n",
    "mask_batch_size = 64\n",
    "# num_heads = 1\n",
    "num_heads = 4\n",
    "\n",
    "# experiment args\n",
    "# exp_dir = '../../exps/cosmogrid'\n",
    "exp_dir = '../../exps/cosmogrid_4h'\n",
    "os.makedirs(exp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a3e71e3-0fef-4072-a32f-12097a830cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = SOPConfig(\n",
    "    json_file='../configs/cosmogrid.json',\n",
    "    num_heads=num_heads,\n",
    ")\n",
    "\n",
    "backbone_model = CNNModel(config.num_labels)\n",
    "state_dict = torch.load(backbone_model_name)\n",
    "backbone_model.load_state_dict(state_dict=state_dict)\n",
    "processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aef0835b-02b8-40b1-bac9-35a92d9cde77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples used for training: 80000\n",
      "# samples used for validation: 10000\n",
      "# samples used for testing: 10000\n",
      "# total samples: 100000\n",
      "x shape (80000, 66, 66) (10000, 66, 66) (10000, 66, 66)\n",
      "y shape (80000, 6) (10000, 6) (10000, 6)\n",
      "masks shape (80000, 66, 66) (10000, 66, 66) (10000, 66, 66)\n",
      "-- ALL --\n",
      "max 0.7257571922558966\n",
      "min -0.034935039865926346\n",
      "-- SPLIT train --\n",
      "max 0.7257571922558966\n",
      "min -0.034935039865926346\n",
      "Finished loading 80000 train images ... \n",
      "# samples used for training: 80000\n",
      "# samples used for validation: 10000\n",
      "# samples used for testing: 10000\n",
      "# total samples: 100000\n",
      "x shape (80000, 66, 66) (10000, 66, 66) (10000, 66, 66)\n",
      "y shape (80000, 6) (10000, 6) (10000, 6)\n",
      "masks shape (80000, 66, 66) (10000, 66, 66) (10000, 66, 66)\n",
      "-- ALL --\n",
      "max 0.6323861033355062\n",
      "min -0.031224769235240986\n",
      "-- SPLIT val --\n",
      "max 0.6323861033355062\n",
      "min -0.031224769235240986\n",
      "Finished loading 10000 val images ... \n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "train_size, val_size = -1, -1\n",
    "# train_size = 100\n",
    "# val_size = 100\n",
    "train_dataset = CosmogridDataset(data_dir=TRAIN_DATA_DIR, split='train', data_size=train_size,\n",
    "                                 inputs_filename='X_maps_Cosmogrid_100k.npy',\n",
    "                                 labels_filename='y_maps_Cosmogrid_100k.npy',\n",
    "                                 mask_path=mask_path,\n",
    "                                num_masks_max=config.num_masks_max)\n",
    "val_dataset = CosmogridDataset(data_dir=TRAIN_DATA_DIR, split='val', data_size=val_size,\n",
    "                               inputs_filename='X_maps_Cosmogrid_100k.npy',\n",
    "                                 labels_filename='y_maps_Cosmogrid_100k.npy',\n",
    "                                 mask_path=mask_path,\n",
    "                                num_masks_max=config.num_masks_max)\n",
    "\n",
    "# Use subset for testing purpose\n",
    "# num_data = 100\n",
    "# train_dataset = Subset(train_dataset, range(num_data))\n",
    "# val_dataset = Subset(val_dataset, range(num_data))\n",
    "\n",
    "# Create a DataLoader to batch and shuffle the data\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e4b54a6-80d5-4bfd-ad56-39a862f28d09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backbone_model = backbone_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bf3ed2f-bae5-4a0a-8481-392821c34cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep copy class weights\n",
      "No weights have changed after initialization.\n"
     ]
    }
   ],
   "source": [
    "model = SOPImageCls(config, backbone_model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d698838c-d435-4bad-b200-9c0d0a4701d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "train_rep_step_size = int(num_training_steps / config.num_heads)\n",
    "lr_scheduler = get_inverse_sqrt_with_separate_heads_schedule_with_warmup(\n",
    "            optimizer=optimizer, \n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_steps_per_epoch=train_rep_step_size,\n",
    "            num_heads=config.num_heads\n",
    "        )\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4128a754-4503-41c2-8970-a1d432844876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval(model, dataloader, criterion, postprocess=lambda x:x):\n",
    "    print('Eval ...')\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        progress_bar_eval = tqdm(range(len(dataloader)))\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # Now you can use `inputs` and `labels` in your training loop.\n",
    "            inputs, labels, masks, _ = batch\n",
    "            inputs, labels = inputs.to(device, dtype=torch.float), labels.to(device, dtype=torch.float)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            logits = postprocess(outputs)\n",
    "            \n",
    "            # val loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            \n",
    "            progress_bar_eval.update(1)\n",
    "    \n",
    "    # val_acc = correct / total\n",
    "    val_loss = total_loss / total\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return {\n",
    "        # 'val_acc': val_acc,\n",
    "        'val_loss': val_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "517fdfee-2e36-41ce-b3fc-8813f407b1f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5d01e29fd74ed6b1ca60b3fdd00736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.008118031978979708"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone_val_results = eval(backbone_model, val_dataloader, criterion, postprocess=lambda x:x.logits)\n",
    "backbone_val_loss = backbone_val_results['val_loss']\n",
    "backbone_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae07940a-296b-478b-bc20-691898613b40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfallcat\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/shared_data0/weiqiuy/sop/notebooks/cosmogrid/wandb/run-20231107_164924-djj31stp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fallcat/sop/runs/djj31stp' target=\"_blank\">golden-waterfall-11</a></strong> to <a href='https://wandb.ai/fallcat/sop' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fallcat/sop' target=\"_blank\">https://wandb.ai/fallcat/sop</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fallcat/sop/runs/djj31stp' target=\"_blank\">https://wandb.ai/fallcat/sop/runs/djj31stp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1680d86c19ad43e9a6e0aa31a4e76597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 100, Loss 0.1137, LR 0.00002475\n",
      "Epoch 0, Batch 200, Loss 0.0824, LR 0.00004975\n",
      "Epoch 0, Batch 300, Loss 0.0327, LR 0.00007475\n",
      "Epoch 0, Batch 400, Loss 0.0250, LR 0.00009975\n",
      "Epoch 0, Batch 500, Loss 0.0203, LR 0.00012475\n",
      "Epoch 0, Batch 600, Loss 0.0204, LR 0.00014975\n",
      "Epoch 0, Batch 700, Loss 0.0194, LR 0.00017475\n",
      "Epoch 0, Batch 800, Loss 0.0194, LR 0.00019975\n",
      "Epoch 0, Batch 900, Loss 0.0184, LR 0.00022475\n",
      "Epoch 0, Batch 1000, Loss 0.0178, LR 0.00024975\n",
      "Eval ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48d5ee5585b41da9fb0645bc7b11293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "track = True\n",
    "# track = False\n",
    "early_stop = False\n",
    "early_stop_met = False\n",
    "\n",
    "if track:\n",
    "    import wandb\n",
    "    wandb.init(project='sop')\n",
    "    wandb.run.name = os.path.basename(exp_dir)\n",
    "\n",
    "# Iterate over the data\n",
    "best_val_loss = np.inf\n",
    "step = 0\n",
    "train_log_interval = 100\n",
    "val_eval_interval = 1000\n",
    "\n",
    "logging.basicConfig(filename=os.path.join(exp_dir, 'train.log'), level=logging.INFO)\n",
    "\n",
    "model.train()\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_total = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        inputs, labels, masks, _ = batch\n",
    "        inputs, labels = inputs.to(device, dtype=torch.float), labels.to(device, dtype=torch.float)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_rep_step = step // train_rep_step_size\n",
    "        logits = model(inputs, segs=masks, epoch=train_rep_step, mask_batch_size=mask_batch_size)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        running_total += labels.size(0)\n",
    "        \n",
    "        if i % train_log_interval == train_log_interval - 1 or i == len(train_dataloader) - 1:\n",
    "            # Print training loss every 100 batches\n",
    "            curr_lr = float(optimizer.param_groups[0]['lr'])\n",
    "            log_message = f'Epoch {epoch}, Batch {i + 1}, Loss {running_loss / running_total:.4f}, LR {curr_lr:.8f}'\n",
    "            print(log_message)\n",
    "            logging.info(log_message)\n",
    "            if track:\n",
    "                wandb.log({'train_loss': running_loss / running_total,\n",
    "                        'lr': curr_lr,\n",
    "                        'epoch': epoch,\n",
    "                        'step': step})\n",
    "            running_loss = 0.0\n",
    "            running_total = 0\n",
    "            \n",
    "        if i % val_eval_interval == val_eval_interval - 1 or i == len(train_dataloader) - 1:\n",
    "            val_results = eval(model, val_dataloader, criterion)\n",
    "            val_loss = val_results['val_loss']\n",
    "            log_message = f'Epoch {epoch}, Step {step}, Val loss {val_loss:.4f}'\n",
    "            print(log_message)\n",
    "            logging.info(log_message)\n",
    "            if track:\n",
    "                wandb.log({\n",
    "                    # 'val_acc': val_acc,\n",
    "                           'val_loss': val_loss,\n",
    "                        'epoch': epoch,\n",
    "                        'step': step})\n",
    "            \n",
    "            last_dir = os.path.join(exp_dir, 'last')\n",
    "            best_dir = os.path.join(exp_dir, 'best')\n",
    "            checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'step': step,\n",
    "                    'val_loss': val_loss,\n",
    "                }\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                os.makedirs(best_dir, exist_ok=True)\n",
    "                best_checkpoint_path = os.path.join(best_dir, 'checkpoint.pth')\n",
    "                torch.save(checkpoint, best_checkpoint_path)\n",
    "                config_best_checkpoint_path = os.path.join(best_dir, 'config.json')\n",
    "                config.save_to_json(config_best_checkpoint_path)\n",
    "                print(f'Best checkpoint saved at {best_checkpoint_path}')\n",
    "                \n",
    "            os.makedirs(last_dir, exist_ok=True)\n",
    "            last_checkpoint_path = os.path.join(last_dir, 'checkpoint.pth')\n",
    "            torch.save(checkpoint, last_checkpoint_path)\n",
    "            config_last_checkpoint_path = os.path.join(last_dir, 'config.json')\n",
    "            config.save_to_json(config_best_checkpoint_path)\n",
    "            print(f'Last checkpoint saved at {last_checkpoint_path}')\n",
    "            \n",
    "            if early_stop and val_loss <= backbone_val_loss:\n",
    "                early_stop_met = True\n",
    "                break\n",
    "            \n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "    if early_stop_met:\n",
    "        break\n",
    "        \n",
    "model.save(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901f3ce2-ba44-404d-96ff-0966cb40e294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bbe74c-7625-4004-ad95-c64645a3f5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
