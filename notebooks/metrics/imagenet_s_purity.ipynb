{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "879c952a-2dee-406f-857c-df9ba155692b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': {'name': 'imagenet_s',\n",
       "  'root': '/shared_data0/weiqiuy/datasets/imagenet'},\n",
       " 'training': {'batch_size': 16,\n",
       "  'num_epochs': 20,\n",
       "  'mask_batch_size': 64,\n",
       "  'optimizer': {'name': 'adamw', 'lr': 5e-06, 'weight_decay': 0.01}},\n",
       " 'evaluation': {'split': 'val', 'num_data': 1, 'batch_size': 16},\n",
       " 'model': {'type': 'vit',\n",
       "  'base': 'google/vit-base-patch16-224',\n",
       "  'sop': '/shared_data0/weiqiuy/sop/exps/imagenet_lr5e-06_tgtnnz0.2_gg0.0600_gs0.0100_ft_identify_fixk_scratch_ks3/best',\n",
       "  'num_classes': 1000}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../../lib/exlib/src')\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "import sop\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sop.utils.seed_all(42)\n",
    "\n",
    "# config\n",
    "exp_config = sop.ImageNetConfig()\n",
    "val_config = exp_config.get_config('val_sm')\n",
    "val_config['evaluation']['batch_size'] = 16\n",
    "val_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217f70e3-ded5-47df-9d6a-3318b488d710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projection layer is not frozen\n",
      "projection layer is not frozen\n",
      "Loaded step 40100\n"
     ]
    }
   ],
   "source": [
    "backbone_model, original_model, processor, backbone_config, model, config = sop.tasks.imagenet.get_model(val_config['model']['type'],\n",
    "                                                                 backbone_model_name=val_config['model']['base'],\n",
    "                                                                 backbone_processor_name=val_config['model']['base'],\n",
    "                                                                 sop_model_name=val_config['model']['sop'], \n",
    "                                                                                                         # sop_model_name='/shared_data0/weiqiuy/sop/exps/imagenet_lr5e-06_tgtnnz0.2_gg0.0600_gs0.0100_ft_identify_fixk_scratch_ks3/last',\n",
    "                                                                                                         eval_mode=True\n",
    "                                                                                                        )\n",
    "\n",
    "backbone_model = backbone_model.to(device)\n",
    "original_model = original_model.to(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15dadf67-c88c-453c-9487-7fcd997f797e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sop.utils.metric_utils import get_entropy, get_prob_obj, get_prob_obj_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd254aeb-d379-4157-8484-3d6c2f30229a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b0954ef1da4bb8b1610b06456f8ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 919 images and 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc04c89066f4ca7a38c79c62319aa13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_430/1092342025.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mmasks_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     results[method] = {\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;34m'entropies'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mentropies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;34m'ratios_obj_coverage'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mratios_obj_coverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sop.metrics import get_acc\n",
    "from sop.tasks.images.imagenet import get_explainer\n",
    "\n",
    "methods = [\n",
    "    # 'bcos',\n",
    "    # 'xdnn',\n",
    "    'bagnet',\n",
    "    # 'sop',\n",
    "    # 'shap_20',\n",
    "    # 'rise_20',\n",
    "    # 'lime_20',\n",
    "    # 'fullgrad',\n",
    "    # 'gradcam',\n",
    "    # 'intgrad',\n",
    "    # 'attn',\n",
    "    # 'archipelago',\n",
    "    # 'mfaba',\n",
    "    # 'agi',\n",
    "    # 'ampe',\n",
    "]\n",
    "\n",
    "\n",
    "debug = False\n",
    "k = 0.2\n",
    "\n",
    "# method = 'lime_20'\n",
    "# explainer_name = method.split('_')[0]\n",
    "# method = 'shap_20'\n",
    "results_all = {}\n",
    "\n",
    "for method in methods:\n",
    "\n",
    "    if method == 'sop':\n",
    "        explainer = model\n",
    "    else:\n",
    "        explainer = get_explainer(original_model, backbone_model, method.split('_')[0], device)\n",
    "\n",
    "    method_list = method.split('_')\n",
    "    explainer_name = method_list[0]\n",
    "\n",
    "    if len(method_list) == 2:\n",
    "        suffix = f'_{method_list[1]}'\n",
    "    else:\n",
    "        suffix = ''\n",
    "\n",
    "    if method not in ['sop', 'bagnet']: #, 'xdnn', 'bagnet']:\n",
    "        ATTR_VAL_DATA_DIR = f'/shared_data0/weiqiuy/sop/exps/imagenet_vit_1/attributions_seg/{explainer_name}_1_pred{suffix}/val'\n",
    "    else:\n",
    "        ATTR_VAL_DATA_DIR = None\n",
    "    \n",
    "    val_dataset, val_dataloader = sop.tasks.imagenet.get_dataset(val_config['dataset']['name'], \n",
    "                                              split=val_config['evaluation']['split'], \n",
    "                                              num_data=val_config['evaluation']['num_data'],\n",
    "                                              batch_size=val_config['evaluation']['batch_size'],\n",
    "                                                            attr_dir=ATTR_VAL_DATA_DIR,\n",
    "                                              processor=processor, debug=debug)\n",
    "\n",
    "    # for k in tqdm(np.linspace(0.1, 1, 10)):\n",
    "    k = 0.2\n",
    "    # results = get_acc(val_dataloader, explainer, method, device, k=k, eval_all=True, built_in=True)\n",
    "    \n",
    "    entropies = []\n",
    "    ratios_obj_coverage = []\n",
    "    ratios_obj = []\n",
    "    accs = []\n",
    "    for bi, batch in tqdm(enumerate(val_dataloader), total=len(val_dataloader)):\n",
    "        if len(batch) == 5:\n",
    "            inputs, labels, segs, attrs, idxs = batch\n",
    "        else:\n",
    "            inputs, labels, segs, idxs = batch\n",
    "        inputs, labels, segs = inputs.to(device), labels.to(device), segs.to(device)\n",
    "        # with torch.no_grad():\n",
    "        #     original_logits = original_model(inputs)\n",
    "        # preds = torch.argmax(original_logits, dim=-1)\n",
    "        # Get explanation\n",
    "        # expln = explainer(inputs, preds)\n",
    "\n",
    "        # if method != 'backbone':\n",
    "        \n",
    "        masks_all = []\n",
    "        for idx in range(len(inputs)):\n",
    "            if method == 'bagnet':\n",
    "                # print('recompute')\n",
    "                expln = explainer(inputs[idx:idx+1], return_groups=True)\n",
    "                masks = expln.group_masks[0]\n",
    "                mask_weights = expln.group_attributions[0].flatten()\n",
    "                \n",
    "                # Sort the masks based on mask_weights\n",
    "                sort_idxs = torch.argsort(mask_weights)\n",
    "                masks = masks[sort_idxs]  # Sort masks accordingly\n",
    "                mask_weights = mask_weights[sort_idxs]\n",
    "\n",
    "                # Cumulative sum of sorted masks\n",
    "                topk = int(masks.shape[-1] * masks.shape[-2] * k)\n",
    "                masks_cumsum = masks.cumsum(dim=0).bool().float()\n",
    "                # print('masks_cumsum', masks_cumsum.shape)\n",
    "                # Calculate the sum along the last two dimensions\n",
    "                masks_cumsum_sum = masks_cumsum.sum((-1, -2))\n",
    "                # print('masks_cumsum_sum', masks_cumsum_sum.shape)\n",
    "\n",
    "                # Find the index where the sum first exceeds topk\n",
    "                mask_index = torch.searchsorted(masks_cumsum_sum, topk)\n",
    "\n",
    "                # Ensure we don't exceed the tensor's first dimension\n",
    "                mask_index = torch.clamp(mask_index, max=masks_cumsum.shape[0] - 1)\n",
    "\n",
    "                mask = masks_cumsum[mask_index]\n",
    "            else:\n",
    "                # Create a mask of size (28, 28) with values from 1 to 28*28\n",
    "                cell_size = 14\n",
    "                image_size = 224\n",
    "                mask = torch.arange(1, cell_size*cell_size + 1, dtype=torch.int).reshape(cell_size, cell_size)\n",
    "\n",
    "                # Resize the mask to (224, 224) without using intermediate floating point numbers\n",
    "                # This can be achieved by repeating each value in both dimensions to scale up the mask\n",
    "                scale_factor = image_size // cell_size  # Calculate scale factor\n",
    "                resized_mask = mask.repeat_interleave(scale_factor, dim=0).repeat_interleave(scale_factor, dim=1)\n",
    "\n",
    "                masks = convert_idx_masks_to_bool(resized_mask[None]).to(device)\n",
    "                mask_weights = (masks.to(device) * attrs[idx][0:1].to(device)).sum(-1).sum(-1).to(device)\n",
    "                sort_idxs = torch.argsort(mask_weights).flip(-1)\n",
    "                masks = masks[sort_idxs]\n",
    "                mask_weights = mask_weights[sort_idxs]\n",
    "\n",
    "                topk = int(masks.shape[0] * k)\n",
    "                masks_use = masks[:topk]\n",
    "                mask = masks_use.sum(0)\n",
    "\n",
    "            # metrics\n",
    "            entropy = get_entropy(mask[None], segs[idx])\n",
    "            entropies.append(entropy.item())\n",
    "            ratios_obj_coverage.extend(get_prob_obj_coverage(mask[None], segs[idx]).view(-1).cpu().numpy().tolist())\n",
    "            ratios_obj.extend(get_prob_obj(mask[None], segs[idx]).view(-1).cpu().numpy().tolist())\n",
    "\n",
    "            masks_all.append(mask)\n",
    "\n",
    "        masks_all = torch.stack(masks_all, dim=0)\n",
    "\n",
    "    results_all[method] = {\n",
    "        'entropies': entropies,\n",
    "        'ratios_obj_coverage': ratios_obj_coverage,\n",
    "        'ratios_obj': ratios_obj,\n",
    "        # 'accs': accs\n",
    "    }\n",
    "    \n",
    "    # results_all[method] = results\n",
    "    print(method, results_all[method]['acc'])\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "005cf904-6c92-4aa3-9859-f61cc7fe681a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_all[method] = {\n",
    "        'entropies': entropies,\n",
    "        'ratios_obj_coverage': ratios_obj_coverage,\n",
    "        'ratios_obj': ratios_obj,\n",
    "        # 'accs': accs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a57c1ed-7903-4f00-b5a0-747718e1ae06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropies 0.6845371479450262\n",
      "ratios_obj_coverage 0.2725739830633513\n",
      "ratios_obj 0.3585904936863047\n"
     ]
    }
   ],
   "source": [
    "for k in results_all[method]:\n",
    "    print(k, np.mean(results_all[method][k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec4f374-da36-4e44-8789-aec09e836bce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.save(results_all, 'bagnet_purity.pt')\n",
    "results_all = torch.load('bagnet_purity.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3080b617-7586-484b-b892-71730c0be179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
