{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9a09585-43de-4080-8030-f74fe225e231",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': {'name': 'imagenet_s',\n",
       "  'root': '/shared_data0/weiqiuy/datasets/imagenet'},\n",
       " 'training': {'batch_size': 16,\n",
       "  'num_epochs': 20,\n",
       "  'mask_batch_size': 64,\n",
       "  'optimizer': {'name': 'adamw', 'lr': 5e-06, 'weight_decay': 0.01}},\n",
       " 'evaluation': {'split': 'val', 'num_data': 1, 'batch_size': 16},\n",
       " 'model': {'type': 'vit',\n",
       "  'base': 'google/vit-base-patch16-224',\n",
       "  'sop': '/shared_data0/weiqiuy/sop/exps/imagenet_lr5e-06_tgtnnz0.2_gg0.0600_gs0.0100_ft_identify_fixk_scratch_ks3/best',\n",
       "  'num_classes': 1000}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../../lib/exlib/src')\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "import sop\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sop.utils.seed_all(42)\n",
    "\n",
    "# config\n",
    "exp_config = sop.ImageNetConfig()\n",
    "val_config = exp_config.get_config('val_sm')\n",
    "val_config['evaluation']['batch_size'] = 16\n",
    "val_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f3ad36-6249-42e5-98e1-3df5a519df62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projection layer is not frozen\n",
      "projection layer is not frozen\n",
      "Loaded step 40100\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "backbone_model, processor, backbone_config = sop.utils.imagenet_utils.get_model(val_config['model']['type'],\n",
    "                                                                 backbone_model_name=val_config['model']['base'],\n",
    "                                                                 backbone_processor_name=val_config['model']['base'],\n",
    "                                                                )\n",
    "backbone_model = backbone_model.to(device)\n",
    "\n",
    "# get wrapped original model\n",
    "from sop.utils.imagenet_utils import WrappedModel\n",
    "\n",
    "original_model = WrappedModel(backbone_model, output_type='logits')\n",
    "original_model = original_model.to(device)\n",
    "original_model.eval();\n",
    "\n",
    "# config\n",
    "from exlib.modules.sop import SOPConfig, get_chained_attr\n",
    "\n",
    "config = SOPConfig(os.path.join(val_config['model']['sop'], 'config.json'))\n",
    "\n",
    "# config.group_sel_scale = 0.05\n",
    "\n",
    "config.__dict__.update(backbone_config.__dict__)\n",
    "config.num_labels = len(backbone_config.id2label)\n",
    "\n",
    "# get sop model\n",
    "from sop.utils.imagenet_utils import get_model, get_wrapped_models\n",
    "\n",
    "wrapped_backbone_model, class_weights, projection_layer = get_wrapped_models(\n",
    "    backbone_model,\n",
    "    config\n",
    ")\n",
    "wrapped_backbone_model = wrapped_backbone_model.to(device)\n",
    "wrapped_backbone_model.eval();\n",
    "class_weights = class_weights.to(device)\n",
    "projection_layer = projection_layer.to(device)\n",
    "\n",
    "# sop\n",
    "from exlib.modules.sop import SOPImageCls4\n",
    "\n",
    "model = SOPImageCls4(config, wrapped_backbone_model, \n",
    "                     class_weights=class_weights, \n",
    "                     projection_layer=projection_layer)\n",
    "state_dict = torch.load(os.path.join(val_config['model']['sop'], \n",
    "                                     'checkpoint.pth'))\n",
    "print('Loaded step', state_dict['step'])\n",
    "model.load_state_dict(state_dict['model'], strict=False)\n",
    "model = model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adc93f3d-2a04-4a7b-aa68-96918dd6f442",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/shared_data0/weiqiuy/sop/exps/imagenet_lr5e-06_tgtnnz0.2_gg0.0600_gs0.0100_ft_identify_fixk_scratch_ks3/best'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_config['model']['sop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a6141a7-3b0a-4664-aa7f-496138d982c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\n",
    "    'shap_20',\n",
    "    'rise_20',\n",
    "    'lime_20',\n",
    "    'sop',\n",
    "    'fullgrad',\n",
    "    'gradcam',\n",
    "    'intgrad',\n",
    "    'attn',\n",
    "    'archipelago',\n",
    "    'mfaba',\n",
    "    'agi',\n",
    "    'ampe',\n",
    "    'bcos',\n",
    "    'xdnn',\n",
    "    'bagnet',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c158e32-409b-4c3f-a253-fcde58527d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sop.metrics import get_ins_del_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ea7cb18-6f19-4d33-a99f-f4b5980aac2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "method = 'shap_20'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc8242-04b3-47ba-960c-7b3357318f68",
   "metadata": {},
   "source": [
    "# get results example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c67d73c-d5c4-4ba4-aaf5-56d4426ff268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shap_20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68dbebc0c0584ead81eef8eec0360d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 images and 100 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6b12d24abe46a1bd91bc0f637ed700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8887572074515773\n"
     ]
    }
   ],
   "source": [
    "# results_ins = {}\n",
    "# for method in methods:\n",
    "results_ins = get_ins_del_perc(val_config, original_model, backbone_model, model, processor,\n",
    "                     method, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1486c92-86a5-4e4b-b0c6-0045afd4004f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shap_20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997b1826ae8c4fcbb75a6ada0f93de58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 images and 100 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b8501d619e4aeab0e5449ae825f63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5132429386690466\n"
     ]
    }
   ],
   "source": [
    "results_del = get_ins_del_perc(val_config, original_model, backbone_model, model, processor,\n",
    "                     method, debug=True, deletion=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c5c418-475e-4076-bd8c-a793d9b1f3d3",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab9f05-3f62-489d-b022-3ea9037bf132",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\n",
    "    'shap_20',\n",
    "    'rise_20',\n",
    "    'lime_20',\n",
    "    'sop',\n",
    "    'fullgrad',\n",
    "    'gradcam',\n",
    "    'intgrad',\n",
    "    'attn',\n",
    "    'archipelago',\n",
    "    'mfaba',\n",
    "    'agi',\n",
    "    'ampe',\n",
    "    'bcos',\n",
    "    'xdnn',\n",
    "    'bagnet',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c35cb71-c1eb-4dfd-b288-657804bada3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n",
      "shap_20 ins 0.8887572074515773 del 0.5132429386690466\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/shared_data0/weiqiuy/sop/results/ins_del/imagenet_s/rise_20.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3499/3035536612.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmethods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/shared_data0/weiqiuy/sop/results/ins_del/imagenet_s/{method}.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ins'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scores_curve_perc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'del'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scores_curve_perc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/shared_data0/weiqiuy/sop/results/ins_del/imagenet_s/rise_20.pt'"
     ]
    }
   ],
   "source": [
    "for method in methods:\n",
    "    data = torch.load(f'/shared_data0/weiqiuy/sop/results/ins_del/imagenet_s/{method}.pt')\n",
    "    print(len(data['ins']['scores_curve_perc']))\n",
    "    print(len(data['del']['scores_curve_perc']))\n",
    "    \n",
    "    print(method, 'ins', np.mean(data['ins']['scores_curve_perc']), \n",
    "          'del', np.mean(data['del']['scores_curve_perc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab11e7-33f7-4b01-a397-9b2553c7a676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba7c3a6-b6ba-490e-9305-c8c0d5aaa6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5349de0a-68be-4756-8c2c-d1e22a11959c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = torch.load('/shared_data0/weiqiuy/sop/results/ins_del/imagenet_s/lime_20.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df1bc832-bba2-4d5d-afd2-5608c56ccce7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['ins', 'del']),\n",
       " dict_keys(['scores_mean', 'scores_curve', 'scores_curve_perc']),\n",
       " [0.30941758659424384,\n",
       "  0.5342400458198,\n",
       "  0.7519785941593089,\n",
       "  0.5726131650617945,\n",
       "  0.982614019327433,\n",
       "  0.893907126682252,\n",
       "  0.8472534268134003,\n",
       "  0.9473662766869041,\n",
       "  0.8753398389306111,\n",
       "  0.9279017636587231])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys(), data['ins'].keys(), data['ins']['scores_curve_perc'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efc04aad-5493-4053-bcf2-5f40ef76e6bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8209578853120209"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(data['ins']['scores_curve_perc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d860e-a830-47f6-8bae-748a860b4862",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in results_ins['scores_curve_perc']:\n",
    "    print(k, np.mean(results_ins['scores_curve_perc'][k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f880a784-5599-4868-85c6-6a89a07d0ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_del = []\n",
    "for explainer_name in methods:\n",
    "    results_del[method] = get_ins_del_perc(val_config, original_model, backbone_model, model, processor,\n",
    "                     explainer_name, debug=True, deletion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566269c5-976d-496c-8af1-15a8d885193e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in results_del['scores_curve_perc']:\n",
    "    print(k, np.mean(results_del['scores_curve_perc'][k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b0227-0827-43e8-a903-3d179242a79d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b242d3-3f07-46df-a677-fab8eb50a1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee8884-ab1e-43cf-bea5-81e7cee5b13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95e529-d540-49c7-b773-053f88cfece9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac30b4e9-5529-4e49-8bc1-4e9108ee36ca",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3f9fce9-2bb9-4404-b904-0acd9ea8d4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_acc(explainer_name, suffix='', debug=False):\n",
    "    method = explainer_name.split('_')[0]\n",
    "    if explainer_name == 'bagnet':\n",
    "        ATTR_VAL_DATA_DIR = None\n",
    "    else:\n",
    "        ATTR_VAL_DATA_DIR = f'/shared_data0/weiqiuy/sop/exps/imagenet_vit_1/attributions_seg/{explainer_name.replace(\"-s\", \"\")}_1_pred{suffix}/val'\n",
    "\n",
    "    explainer = sop.utils.get_explainer(original_model, backbone_model, method.split('_')[0], device)\n",
    "\n",
    "    val_dataset, val_dataloader = sop.utils.get_dataset(val_config['dataset']['name'], \n",
    "                                          split=val_config['evaluation']['split'], \n",
    "                                          num_data=val_config['evaluation']['num_data'],\n",
    "                                          batch_size=val_config['evaluation']['batch_size'],\n",
    "                                                        attr_dir=ATTR_VAL_DATA_DIR,\n",
    "                                          processor=processor, debug=debug)\n",
    "\n",
    "    corrects = []\n",
    "    for bi, batch in tqdm(enumerate(val_dataloader), total=len(val_dataloader)):\n",
    "        if debug and bi >= 3:\n",
    "            break\n",
    "        # if bi != len(val_dataloader) - 1:\n",
    "        #     continue\n",
    "        if len(batch) == 5:\n",
    "            inputs, labels, segs, attrs, idxs = batch\n",
    "        else:\n",
    "            inputs, labels, segs, idxs = batch\n",
    "            attrs = None\n",
    "        inputs, labels, segs = inputs.to(device), labels.to(device), segs.to(device)\n",
    "\n",
    "        inputs_norm = inputs\n",
    "        # inputs_norm = (inputs_norm + 1) / 2\n",
    "        inputs_norm = explainer.preprocess(inputs_norm)\n",
    "        print('inputs_norm', inputs_norm.shape)\n",
    "        # inputs_norm = (inputs_norm + 1) / 2\n",
    "        outputs = explainer.model(inputs_norm)\n",
    "        probs = outputs.softmax(-1)\n",
    "        preds = probs.argmax(-1)\n",
    "        print(preds, labels, preds == labels)\n",
    "        corrects.extend((preds == labels).cpu().tolist())\n",
    "    print(sum(corrects) / len(corrects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f45f67-1dbf-4dd9-83a3-468b6aee0123",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/runai-home/.cache/torch/hub/B-cos_B-cos-v2_main\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9692244841d4f96b2b1ab395464479c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 images and 100 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf9827d3d254417bcfd01d5e88c8e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_norm torch.Size([1, 6, 224, 224])\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0') tensor([True], device='cuda:0')\n",
      "inputs_norm torch.Size([1, 6, 224, 224])\n",
      "tensor([1], device='cuda:0') tensor([1], device='cuda:0') tensor([True], device='cuda:0')\n",
      "inputs_norm torch.Size([1, 6, 224, 224])\n",
      "tensor([2], device='cuda:0') tensor([2], device='cuda:0') tensor([True], device='cuda:0')\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "get_acc('bcos', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d635a-fc01-4a06-9419-d8fc62b2a4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
