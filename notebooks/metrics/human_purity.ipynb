{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e187b14b-e4d6-4e3e-8f99-ba379ff2ec5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, AutoConfig\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import sys\n",
    "sys.path.append('../../lib/exlib/src')\n",
    "# from exlib.modules.sop import SOPImageCls, SOPConfig, get_chained_attr\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "SEED = 42\n",
    "if SEED != -1:\n",
    "    # Torch RNG\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    # Python RNG\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    \n",
    "# model paths\n",
    "backbone_model_name = 'google/vit-base-patch16-224'\n",
    "backbone_processor_name = 'google/vit-base-patch16-224'\n",
    "\n",
    "# data paths\n",
    "# TRAIN_DATA_DIR = '../data/imagenet_m/train'\n",
    "# VAL_DATA_DIR = '../data/imagenet_m/val'\n",
    "TRAIN_DATA_DIR = '/scratch/datasets/imagenet/train'\n",
    "VAL_DATA_DIR = '/scratch/datasets/imagenet/val'\n",
    "\n",
    "\n",
    "# training args\n",
    "batch_size = 16\n",
    "lr = 0.000005\n",
    "num_epochs = 20\n",
    "warmup_steps = 2000\n",
    "mask_batch_size = 64\n",
    "\n",
    "from exlib.modules.sop import get_chained_attr\n",
    "\n",
    "backbone_model = AutoModelForImageClassification.from_pretrained(backbone_model_name)\n",
    "processor = AutoImageProcessor.from_pretrained(backbone_processor_name)\n",
    "backbone_config = AutoConfig.from_pretrained(backbone_model_name)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def resize_binary_image(image, size, mode='bilinear'):\n",
    "    # Resize the image\n",
    "    resized_image = F.interpolate(image, size=size, mode=mode, align_corners=False)\n",
    "\n",
    "    # Threshold the image to convert values back to binary (0 or 1)\n",
    "    thresholded_image = (resized_image > 0.5).float()\n",
    "\n",
    "    return thresholded_image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "class ImageFolderSubDataset(Dataset):\n",
    "    def __init__(self, data_dir, attr_dir, transform=None, num_data=-1, start_data=0):\n",
    "        self.data_dir = data_dir\n",
    "        self.attr_dir = attr_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.attr_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for label in sorted(os.listdir(data_dir)):\n",
    "            dirname = os.path.join(data_dir, label)\n",
    "            if not os.path.isdir(dirname):\n",
    "                continue\n",
    "            for i, image_path in enumerate(sorted(os.listdir(dirname))):\n",
    "                if i < start_data:\n",
    "                    continue\n",
    "                if num_data != -1 and i >= num_data:\n",
    "                    break\n",
    "                attr_path = os.path.join(self.attr_dir, label, os.path.basename(image_path) + '.pt')\n",
    "                if not os.path.exists(attr_path):\n",
    "                    print('failed' + attr_path)\n",
    "                    continue\n",
    "                self.image_paths.append(os.path.join(data_dir, label, image_path))\n",
    "                self.attr_paths.append(attr_path)\n",
    "                self.labels.append(label)\n",
    "        self.all_labels = sorted(list(set(self.labels)))\n",
    "        \n",
    "        print('Loaded {} images and {} classes'.format(len(self.image_paths), len(self.all_labels)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        attr_path = self.attr_paths[idx]\n",
    "        label = self.all_labels.index(self.labels[idx])\n",
    "        image = Image.open(image_path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        attr = torch.load(attr_path)\n",
    "        return image, label, attr\n",
    "    \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_masked_img(img, mask, ax):\n",
    "    ax.imshow(img.permute(1,2,0).cpu().numpy())\n",
    "    ax.imshow(mask.cpu().numpy(), cmap='hot', alpha=0.5)\n",
    "    ax.contour(mask.cpu().numpy(), 2, colors='black', linestyles='dashed')\n",
    "    ax.contourf(mask.cpu().numpy(), 2, hatches=['//', None, None],\n",
    "                cmap='gray', extend='neither', linestyles='-', alpha=0.01)\n",
    "    ax.axis('off')\n",
    "    \n",
    "def show_masks(img, masks, titles=None, cols=5, imgsize=3):\n",
    "    n_masks = len(masks)\n",
    "    rows = math.ceil(n_masks / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * imgsize, rows * imgsize))\n",
    "    axes = axes.ravel()\n",
    "    for m_i in range(len(masks)):\n",
    "        mask = masks[m_i]\n",
    "        show_masked_img(img, mask, axes[m_i])\n",
    "        if titles is not None:\n",
    "            axes[m_i].set_title(titles[m_i])\n",
    "    for m_i in range(len(masks), len(axes)):\n",
    "        axes[m_i].axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def get_mask_weights_titles(mask_weights):\n",
    "    titles = [f'{mask_weight.item():.10f}' for mask_weight in mask_weights]\n",
    "    return titles\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "WrappedBackboneOutput = namedtuple(\"WrappedBackboneOutput\", \n",
    "                                  [\"logits\",\n",
    "                                   \"pooler_output\"])\n",
    "\n",
    "\n",
    "class WrappedBackboneModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(inputs, output_hidden_states=True)\n",
    "        return WrappedBackboneOutput(outputs.logits, outputs.hidden_states[-1][:,0])\n",
    "    \n",
    "class WrappedModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(inputs, output_hidden_states=True)\n",
    "        return outputs.logits\n",
    "    \n",
    "import copy\n",
    "# only finetune last layer\n",
    "backbone_model_type = 'vit'\n",
    "if backbone_model_type == 'resnet':\n",
    "    original_model = copy.deepcopy(backbone_model).to(device)\n",
    "else:\n",
    "    original_model = WrappedModel(copy.deepcopy(backbone_model)).to(device)\n",
    "original_model.eval();\n",
    "\n",
    "model = copy.deepcopy(backbone_model).to(device)\n",
    "    \n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for name, param in get_chained_attr(model, 'classifier').named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e145630-9d68-4e77-8116-0e83b279b18d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bagnet\n",
      "Loaded 1000 images and 1000 classes\n",
      "bagnet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8b66089a984af7a6364d32a8eddb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 47.54 GiB total capacity; 45.36 GiB already allocated; 112.94 MiB free; 46.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12314/293583406.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;31m# break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12314/1769030553.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1172\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             _internal=True)\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             untyped_storage = torch.UntypedStorage(\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 47.54 GiB total capacity; 45.36 GiB already allocated; 112.94 MiB free; 46.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from exlib.evaluators.common import convert_idx_masks_to_bool\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "\n",
    "from sop.tasks.images.imagenet import get_explainer\n",
    "\n",
    "def transform(image):\n",
    "    # Preprocess the image using the ViTImageProcessor\n",
    "    image = image.convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors='pt')\n",
    "    return inputs['pixel_values'].squeeze(0)\n",
    "\n",
    "\n",
    "methods = [\n",
    "    # 'archipelago',\n",
    "    # 'attn',\n",
    "    # 'fullgrad',\n",
    "    # 'gradcam',\n",
    "    # 'intgrad',\n",
    "    # 'lime',\n",
    "    # 'lime_20',\n",
    "    # 'rise',\n",
    "    # 'rise_20',\n",
    "    # 'shap',\n",
    "    # 'shap_20'\n",
    "    # 'mfaba',\n",
    "    # 'agi',\n",
    "    # 'ampe',\n",
    "    # 'bcos',\n",
    "    # 'xdnn',\n",
    "    'bagnet'\n",
    "]\n",
    "\n",
    "results_all_als = defaultdict(list)\n",
    "\n",
    "\n",
    "\n",
    "for method in methods:\n",
    "    print(method)\n",
    "    method_list = method.split('_')\n",
    "    explainer_name = method_list[0]\n",
    "    \n",
    "    explainer = get_explainer(original_model, backbone_model, method.split('_')[0], device)\n",
    "    \n",
    "    if len(method_list) == 2:\n",
    "        suffix = '_' + method_list[1]\n",
    "    else:\n",
    "        suffix = ''\n",
    "    # ATTR_VAL_DATA_DIR = f'../exps/imagenet_vit_1/attributions/{explainer_name}_5_pred{suffix}/val'\n",
    "    # val_dataset = ImageFolderSubDataset(VAL_DATA_DIR, attr_dir=ATTR_VAL_DATA_DIR, transform=transform, \n",
    "    #                                     num_data=5, start_data=1)\n",
    "    # val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # ATTR_VAL_DATA_DIR = f'../exps/imagenet_vit_1/attributions/{explainer_name}_5_pred{suffix}/val'\n",
    "    \n",
    "    ATTR_VAL_DATA_DIR = f'../../exps/imagenet_vit_1/attributions/vit_{explainer_name}_1_pred{suffix}/val'\n",
    "    val_dataset = ImageFolderSubDataset(VAL_DATA_DIR, attr_dir=ATTR_VAL_DATA_DIR, transform=transform, \n",
    "                                        num_data=1, start_data=0)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # val_size = 1\n",
    "    # for k in tqdm(np.linspace(0.1, 1, 10)):\n",
    "    # # k = 0.2\n",
    "    #     val_acc_k_results = get_acc(val_dataloader, k=k, eval_all=True)\n",
    "    #     results_all_als[method].append(val_acc_k_results)\n",
    "    # print(results_all_als[method][0]['acc'])\n",
    "    model.eval();\n",
    "    print(explainer_name)\n",
    "    k = 0.2\n",
    "    def show_masks_save(img, masks, titles=None, cols=5, imgsize=3, save_path=None):\n",
    "        n_masks = len(masks)\n",
    "        rows = math.ceil(n_masks / cols)\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols * imgsize, rows * imgsize))\n",
    "        try:\n",
    "            axes = axes.ravel()\n",
    "        except:\n",
    "            axes = [axes]\n",
    "        for m_i in range(len(masks)):\n",
    "            mask = masks[m_i]\n",
    "            show_masked_img(img, mask, axes[m_i])\n",
    "            if titles is not None:\n",
    "                axes[m_i].set_title(titles[m_i])\n",
    "        for m_i in range(len(masks), len(axes)):\n",
    "            axes[m_i].axis('off')\n",
    "        if save_path is not None:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "        plt.show()\n",
    "\n",
    "    for bi, batch in tqdm(enumerate(val_dataloader)):\n",
    "        if bi in [0, 10, 20, 30]:\n",
    "            # break\n",
    "\n",
    "            inputs, labels, attrs = batch\n",
    "            inputs, labels, attrs = inputs.to(device), labels.to(device), attrs.to(device)\n",
    "            with torch.no_grad():\n",
    "                original_outputs = original_model(inputs)\n",
    "                original_logits = original_outputs\n",
    "                original_preds = original_logits.argmax(-1)\n",
    "                # Get explanation\n",
    "                # expln = explainer(inputs, original_preds)\n",
    "\n",
    "            # Get \n",
    "            if len(attrs.shape) == 5:\n",
    "                attrs = attrs.squeeze(-1)\n",
    "            masks_all = []\n",
    "            for idx in range(len(inputs)):\n",
    "                # Create a mask of size (28, 28) with values from 1 to 28*28\n",
    "                if method == 'bagnet':\n",
    "                    print('recompute')\n",
    "                    expln = explainer(inputs[idx:idx+1], return_groups=True)\n",
    "                    masks = expln.group_masks[0]\n",
    "                    mask_weights = expln.group_attributions[0].flatten()\n",
    "                else:\n",
    "                    cell_size = 14\n",
    "                    image_size = 224\n",
    "                    mask = torch.arange(1, cell_size*cell_size + 1, dtype=torch.int).reshape(cell_size, cell_size)\n",
    "\n",
    "                    # Resize the mask to (224, 224) without using intermediate floating point numbers\n",
    "                    # This can be achieved by repeating each value in both dimensions to scale up the mask\n",
    "                    scale_factor = image_size // cell_size  # Calculate scale factor\n",
    "                    resized_mask = mask.repeat_interleave(scale_factor, dim=0).repeat_interleave(scale_factor, dim=1)\n",
    "\n",
    "                    masks = convert_idx_masks_to_bool(resized_mask[None]).to(device)\n",
    "                    mask_weights = (masks.to(device) * attrs[idx][0:1].to(device)).sum(-1).sum(-1).to(device)\n",
    "                sort_idxs = torch.argsort(mask_weights).flip(-1)\n",
    "                masks = masks[sort_idxs]\n",
    "                mask_weights = mask_weights[sort_idxs]\n",
    "\n",
    "                if method == 'bagnet':\n",
    "                    topk = int(masks.shape[-1] * masks.shape[-2] * k)\n",
    "                    masks_cumsum = masks.cumsum(dim=0).bool().float()\n",
    "                    # print('masks_cumsum', masks_cumsum.shape)\n",
    "                    # Calculate the sum along the last two dimensions\n",
    "                    masks_cumsum_sum = masks_cumsum.sum((-1, -2))\n",
    "                    # print('masks_cumsum_sum', masks_cumsum_sum.shape)\n",
    "\n",
    "                    # Find the index where the sum first exceeds topk\n",
    "                    mask_index = torch.searchsorted(masks_cumsum_sum, topk)\n",
    "\n",
    "                    # Ensure we don't exceed the tensor's first dimension\n",
    "                    mask_index = torch.clamp(mask_index, max=masks_cumsum.shape[0] - 1)\n",
    "                    \n",
    "                    mask = masks_cumsum[mask_index]\n",
    "                else:\n",
    "                    topk = int(masks.shape[0] * k)\n",
    "                    masks_use = masks[:topk]\n",
    "                    mask = masks_use.sum(0).bool().float()\n",
    "                # print(k)\n",
    "                # print(masks.shape[0])\n",
    "                # print('topk', topk)\n",
    "                # print(mask.sum() / mask.view(-1).shape[0])\n",
    "\n",
    "                masks_all.append(mask)\n",
    "            \n",
    "            masks_all = torch.stack(masks_all, dim=0)\n",
    "\n",
    "            # Get masked output\n",
    "\n",
    "            masked_inputs = masks_all[:,None] * inputs\n",
    "            # plt.figure()\n",
    "            # plt.imshow(masked_inputs[0].permute(1,2,0))\n",
    "            with torch.no_grad():\n",
    "                outputs = model(masked_inputs)\n",
    "            logits = outputs.logits\n",
    "            preds = logits.argmax(-1)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            root_dir = '../neurips2024_user_study2'\n",
    "            save_dir = f'{root_dir}/imgs'\n",
    "            save_dir_label = f'{root_dir}/labels'\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            os.makedirs(save_dir_label, exist_ok=True)\n",
    "            for idx in range(4):\n",
    "                print(backbone_config.id2label[preds[idx].item()].split(',')[0])\n",
    "                # show_masks_pure(inputs, outputs, idx=idx, k=1)\n",
    "                denormed_img = (inputs[idx:idx+1] + 1) / 2\n",
    "                # print('original')\n",
    "                # plt.figure()\n",
    "                # plt.imshow(denormed_img[0].cpu().permute(1,2,0))\n",
    "                # plt.axis('off')\n",
    "                # plt.savefig(os.path.join(save_dir, f'{bi}_{idx}_original.png'), dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "                # plt.show()\n",
    "                show_masks_save(denormed_img[0], masks_all[idx:idx+1], \n",
    "                                save_path=os.path.join(save_dir, f'{bi}_{idx}_{explainer_name}.png'), cols=1)\n",
    "                # show_masks_weights_save(inputs, outputs, preds, idx=idx, save_path=os.path.join(save_dir, f'{bi}_{idx}_gradcam.png'))\n",
    "                with open(os.path.join(save_dir_label, f'{bi}_{idx}_{explainer_name}.txt'), 'wt') as output_file:\n",
    "                    output_file.write(backbone_config.id2label[preds[idx].item()].split(',')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45360d-e471-49f6-8393-e15648a6da26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
