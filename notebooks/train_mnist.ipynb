{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c901246c-d35a-427c-9023-d272b029d868",
   "metadata": {},
   "source": [
    "# SOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56bcda-a8a8-4dac-bf74-967e07362780",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c6a83af-dc9c-4db7-a19b-92000e29b73e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: optimize\n",
    "\n",
    "from __future__ import division\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import collections.abc\n",
    "from functools import partial\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import copy\n",
    "import json\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "AttributionOutputSOP = namedtuple(\"AttributionOutputSOP\", \n",
    "                                  [\"logits\",\n",
    "                                   \"logits_all\",\n",
    "                                   \"pooler_outputs_all\",\n",
    "                                   \"masks\",\n",
    "                                   \"mask_weights\",\n",
    "                                   \"attributions\", \n",
    "                                   \"attributions_max\",\n",
    "                                   \"attributions_all\",\n",
    "                                   \"flat_masks\"])\n",
    "\n",
    "\n",
    "def get_chained_attr(obj, attr_chain):\n",
    "    attrs = attr_chain.split(\".\")\n",
    "    for attr in attrs:\n",
    "        obj = getattr(obj, attr)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def compress_single_masks(masks, masks_weights, min_size):\n",
    "    # num_masks, seq_len = masks.shape\n",
    "    masks_bool = (masks > 0).int()\n",
    "    sorted_weights, sorted_indices = torch.sort(masks_weights, descending=True)\n",
    "    sorted_indices = sorted_indices[sorted_weights > 0]\n",
    "\n",
    "    masks_bool = masks_bool[sorted_indices]  # sorted masks\n",
    "    \n",
    "    masks = torch.zeros(*masks_bool.shape[1:]).to(masks.device)\n",
    "    count = 1\n",
    "    for mask in masks_bool:\n",
    "        new_mask = mask.bool() ^ (mask.bool() & masks.bool())\n",
    "        if torch.sum(new_mask) >= min_size:\n",
    "            masks[new_mask] = count\n",
    "            count += 1\n",
    "\n",
    "    masks = masks - 1\n",
    "    masks = masks.int()\n",
    "    masks[masks == -1] = torch.max(masks) + 1\n",
    "\n",
    "    return masks\n",
    "\n",
    "def compress_masks(masks, masks_weights, min_size=0):\n",
    "    new_masks = []\n",
    "    for i in range(len(masks)):\n",
    "        compressed_mask = compress_single_masks(masks[i], masks_weights[i], \n",
    "                                                min_size)\n",
    "        new_masks.append(compressed_mask)\n",
    "    return torch.stack(new_masks)\n",
    "\n",
    "\n",
    "def compress_masks_image(masks, masks_weights, min_size=0):\n",
    "    assert len(masks.shape) == 4 # bsz, num_masks, img_dim_1, img_dim_2 = masks.shape\n",
    "    return compress_masks(masks, masks_weights, min_size)\n",
    "    \n",
    "\n",
    "def compress_masks_text(masks, masks_weights, min_size=0):\n",
    "    assert len(masks.shape) == 3 # bsz, num_masks, seq_len = masks.shape\n",
    "    return compress_masks(masks, masks_weights, min_size)\n",
    "           \n",
    "\n",
    "def _get_inverse_sqrt_with_separate_heads_schedule_with_warmup_lr_lambda(\n",
    "    current_step: int, *, num_warmup_steps: int, \n",
    "    num_steps_per_epoch: int,\n",
    "    timescale: int = None, \n",
    "    num_heads: int = 1, \n",
    "):\n",
    "    epoch = current_step // (num_steps_per_epoch * num_heads)\n",
    "    steps_within_epoch = current_step % num_steps_per_epoch\n",
    "    step_for_curr_head = epoch * num_steps_per_epoch + steps_within_epoch\n",
    "    if step_for_curr_head < num_warmup_steps:\n",
    "        return float(step_for_curr_head) / float(max(1, num_warmup_steps))\n",
    "    shift = timescale - num_warmup_steps\n",
    "    decay = 1.0 / math.sqrt((step_for_curr_head + shift) / timescale)\n",
    "    return decay\n",
    "\n",
    "def get_inverse_sqrt_with_separate_heads_schedule_with_warmup(\n",
    "    optimizer: Optimizer, num_warmup_steps: int, num_steps_per_epoch: int,\n",
    "    timescale: int = None, \n",
    "    num_heads: int = 1, last_epoch: int = -1\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n",
    "    initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases\n",
    "    linearly between 0 and the initial lr set in the optimizer.\n",
    "\n",
    "    Args:\n",
    "        optimizer ([`~torch.optim.Optimizer`]):\n",
    "            The optimizer for which to schedule the learning rate.\n",
    "        num_warmup_steps (`int`):\n",
    "            The number of steps for the warmup phase.\n",
    "        num_training_steps (`int`):\n",
    "            The total number of training steps.\n",
    "        num_cycles (`int`, *optional*, defaults to 1):\n",
    "            The number of hard restarts to use.\n",
    "        last_epoch (`int`, *optional*, defaults to -1):\n",
    "            The index of the last epoch when resuming training.\n",
    "\n",
    "    Return:\n",
    "        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    if timescale is None:\n",
    "        timescale = num_warmup_steps\n",
    "\n",
    "    lr_lambda = partial(\n",
    "        _get_inverse_sqrt_with_separate_heads_schedule_with_warmup_lr_lambda,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_steps_per_epoch=num_steps_per_epoch,\n",
    "        timescale=timescale,\n",
    "        num_heads=num_heads,\n",
    "    )\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "\"\"\"Sparsemax activation function.\n",
    "\n",
    "Pytorch implementation of Sparsemax function from:\n",
    "-- \"From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification\"\n",
    "-- André F. T. Martins, Ramón Fernandez Astudillo (http://arxiv.org/abs/1602.02068)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Sparsemax(nn.Module):\n",
    "    \"\"\"Sparsemax function.\"\"\"\n",
    "\n",
    "    def __init__(self, dim=None):\n",
    "        \"\"\"Initialize sparsemax activation\n",
    "        \n",
    "        Args:\n",
    "            dim (int, optional): The dimension over which to apply the sparsemax function.\n",
    "        \"\"\"\n",
    "        super(Sparsemax, self).__init__()\n",
    "\n",
    "        self.dim = -1 if dim is None else dim\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward function.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor. First dimension should be the batch size\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [batch_size x number_of_logits] Output tensor\n",
    "\n",
    "        \"\"\"\n",
    "        # Sparsemax currently only handles 2-dim tensors,\n",
    "        # so we reshape to a convenient shape and reshape back after sparsemax\n",
    "        device = inputs.device\n",
    "        inputs = inputs.transpose(0, self.dim)\n",
    "        original_size = inputs.size()\n",
    "        inputs = inputs.reshape(inputs.size(0), -1)\n",
    "        inputs = inputs.transpose(0, 1)\n",
    "        dim = 1\n",
    "\n",
    "        number_of_logits = inputs.size(dim)\n",
    "\n",
    "        # Translate input by max for numerical stability\n",
    "        inputs = inputs - torch.max(inputs, dim=dim, keepdim=True)[0].expand_as(inputs)\n",
    "\n",
    "        # Sort input in descending order.\n",
    "        # (NOTE: Can be replaced with linear time selection method described here:\n",
    "        # http://stanford.edu/~jduchi/projects/DuchiShSiCh08.html)\n",
    "        zs = torch.sort(input=inputs, dim=dim, descending=True)[0]\n",
    "        range_tensor = torch.arange(start=1, end=number_of_logits + 1, step=1, \n",
    "                                    device=device, dtype=inputs.dtype).view(1, -1)\n",
    "        range_tensor = range_tensor.expand_as(zs)\n",
    "\n",
    "        # Determine sparsity of projection\n",
    "        bound = 1 + range_tensor * zs\n",
    "        cumulative_sum_zs = torch.cumsum(zs, dim)\n",
    "        is_gt = torch.gt(bound, cumulative_sum_zs).type(inputs.type())\n",
    "        k = torch.max(is_gt * range_tensor, dim, keepdim=True)[0]\n",
    "\n",
    "        # Compute threshold function\n",
    "        zs_sparse = is_gt * zs\n",
    "\n",
    "        # Compute taus\n",
    "        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k\n",
    "        taus = taus.expand_as(inputs)\n",
    "\n",
    "        # Sparsemax\n",
    "        self.output = torch.max(torch.zeros_like(inputs), inputs - taus)\n",
    "\n",
    "        # Reshape back to original shape\n",
    "        output = self.output\n",
    "        output = output.transpose(0, 1)\n",
    "        output = output.reshape(original_size)\n",
    "        output = output.transpose(0, self.dim)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Backward function.\"\"\"\n",
    "        dim = 1\n",
    "\n",
    "        nonzeros = torch.ne(self.output, 0)\n",
    "        sum = torch.sum(grad_output * nonzeros, dim=dim) / torch.sum(nonzeros, dim=dim)\n",
    "        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n",
    "\n",
    "        return self.grad_input\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08451e57-58c0-40be-9410-b128ca61f09b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Other layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b4cbb8-46e2-4f02-a770-090b46173a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GroupGenerateLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "       \n",
    "        # self.multihead_attn = nn.MultiheadAttention(hidden_dim, num_heads, \n",
    "        #                                             batch_first=True)\n",
    "        self.multihead_attns = nn.ModuleList([nn.MultiheadAttention(hidden_dim, \n",
    "                                                                   1, \n",
    "                                                                   batch_first=True) \\\n",
    "                                                for _ in range(num_heads)])\n",
    "        self.sparsemax = Sparsemax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key_value, epoch=0):\n",
    "        \"\"\"\n",
    "            Use multiheaded attention to get mask\n",
    "            Num_interpretable_heads = num_heads * seq_len\n",
    "            Input: x (bsz, seq_len, hidden_dim)\n",
    "                   if actual_x is not None, then use actual_x instead of x to compute attn_output\n",
    "            Output: attn_outputs (bsz, num_heads * seq_len, seq_len, hidden_dim)\n",
    "                    mask (bsz, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        epsilon = 1e-30\n",
    "        \n",
    "        if epoch == -1:\n",
    "            epoch = self.num_heads\n",
    "\n",
    "        head_i = epoch % self.num_heads\n",
    "        if self.training:\n",
    "            _, attn_weights = self.multihead_attns[head_i](query, key_value, key_value, \n",
    "                                                          average_attn_weights=False)\n",
    "        else:\n",
    "            attn_weights = []\n",
    "            if epoch < self.num_heads:\n",
    "                num_heads_use = head_i + 1\n",
    "            else:\n",
    "                num_heads_use = self.num_heads\n",
    "            for head_j in range(num_heads_use):\n",
    "                _, attn_weights_j = self.multihead_attns[head_j](query, key_value, key_value)\n",
    "                attn_weights.append(attn_weights_j)\n",
    "            attn_weights = torch.stack(attn_weights, dim=1)\n",
    "        \n",
    "        attn_weights = attn_weights + epsilon\n",
    "        mask = self.sparsemax(torch.log(attn_weights))\n",
    "            \n",
    "        return mask\n",
    "\n",
    "\n",
    "class GroupSelectLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        # self.num_heads = num_heads\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_dim, 1, \n",
    "                                                    batch_first=True)\n",
    "        self.sparsemax = Sparsemax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        \"\"\"\n",
    "            Use multiheaded attention to get mask\n",
    "            Num_heads = num_heads * seq_len\n",
    "            Input: x (bsz, seq_len, hidden_dim)\n",
    "                   if actual_x is not None, then use actual_x instead of x to compute attn_output\n",
    "            Output: attn_outputs (bsz, num_heads * seq_len, seq_len, hidden_dim)\n",
    "                    mask (bsz, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # x shape: (batch_size, sequence_length, hidden_dim)\n",
    "        # x shape: (..., hidden_dim)\n",
    "        epsilon = 1e-30\n",
    "        bsz, seq_len, hidden_dim = query.shape\n",
    "\n",
    "        # Obtain attention weights\n",
    "        _, attn_weights = self.multihead_attn(query, key, key)\n",
    "        attn_weights = attn_weights + epsilon  # (batch_size, num_heads, sequence_length, hidden_dim)\n",
    "        # attn_output: (batch_size, sequence_length, hidden_dim)\n",
    "        # attn_weights: (batch_size, num_heads, sequence_length, hidden_dim)\n",
    "        # attn_weights = attn_weights.mean(dim=-2)  # average seq_len number of heads\n",
    "        # if we do sparsemax directly, they are all within 0 and 1 and thus don't move. \n",
    "        # need to first transform to log space.\n",
    "        mask = self.sparsemax(torch.log(attn_weights))\n",
    "        mask = mask.transpose(-1, -2)\n",
    "\n",
    "        # Apply attention weights on what to be attended\n",
    "        new_shape = list(mask.shape) + [1] * (len(value.shape) - 3)\n",
    "        attn_outputs = (value * mask.view(*new_shape)).sum(1)\n",
    "\n",
    "        # attn_outputs of shape (bsz, num_masks, num_classes)\n",
    "        return attn_outputs, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75911db7-d186-4aec-b482-32d4d0afd253",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SOP Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39735cc4-bcda-4176-9c27-770ed1699cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SOPConfig:\n",
    "    def __init__(self,\n",
    "                 json_file=None,\n",
    "                 hidden_size: int = 512,\n",
    "                 num_labels: int = 2,\n",
    "                 projected_input_scale: int = 1,\n",
    "                 num_heads: int = 1,\n",
    "                 num_masks_sample: int = 20,\n",
    "                 num_masks_max: int = 200,\n",
    "                 image_size=(224, 224),\n",
    "                 num_channels: int = 3,\n",
    "                 attn_patch_size: int = 16,\n",
    "                 finetune_layers=[]):\n",
    "\n",
    "        # all the config from the json file will be in self.__dict__\n",
    "        # super().__init__(**kwargs)\n",
    "\n",
    "        if json_file is not None:\n",
    "            self.update_from_json(json_file)\n",
    "        \n",
    "        # overwrite the config from json file if specified\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_labels = num_labels\n",
    "        self.projected_input_scale = projected_input_scale\n",
    "        self.num_heads = num_heads\n",
    "        self.num_masks_sample = num_masks_sample\n",
    "        self.num_masks_max = num_masks_max\n",
    "        self.image_size = image_size\n",
    "        self.num_channels = num_channels\n",
    "        self.attn_patch_size = attn_patch_size\n",
    "        self.finetune_layers = finetune_layers\n",
    "        \n",
    "    def update_from_json(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            json_dict = json.load(f)\n",
    "        self.__dict__.update(json_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250388d-1f07-4694-b8e5-860883c103fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f43041f-6f2d-4f57-81e3-c91d2d030843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SOP(nn.Module):\n",
    "    config_class = SOPConfig\n",
    "\n",
    "    def __init__(self, \n",
    "                 config,\n",
    "                 backbone_model,\n",
    "                 class_weights\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size  # match black_box_model hidden_size\n",
    "        self.num_classes = config.num_labels if hasattr(config, 'num_labels') is not None else 1  # 1 is for regression\n",
    "        self.projected_input_scale = config.projected_input_scale if hasattr(config, 'projected_input_scale') else 1\n",
    "        self.num_heads = config.num_heads\n",
    "        self.num_masks_sample = config.num_masks_sample\n",
    "        self.num_masks_max = config.num_masks_max\n",
    "        self.finetune_layers = config.finetune_layers\n",
    "\n",
    "        # blackbox model and finetune layers\n",
    "        self.blackbox_model = backbone_model\n",
    "        try:\n",
    "            self.class_weights = copy.deepcopy(class_weights)\n",
    "            print('deep copy class weights')\n",
    "        except:\n",
    "            self.class_weights = class_weights.clone()\n",
    "            print('shallow copy class weights')\n",
    "        \n",
    "        \n",
    "        self.input_attn = GroupGenerateLayer(hidden_dim=self.hidden_size,\n",
    "                                             num_heads=self.num_heads)\n",
    "        # output\n",
    "        self.output_attn = GroupSelectLayer(hidden_dim=self.hidden_size)\n",
    "\n",
    "    def init_grads(self):\n",
    "        # Initialize the weights of the model\n",
    "        # blackbox_model = self.blackbox_model.clone()\n",
    "        # self.init_weights()\n",
    "        # self.blackbox_model = blackbox_model\n",
    "\n",
    "        for name, param in self.blackbox_model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for finetune_layer in self.finetune_layers:\n",
    "            # todo: check if this works with index\n",
    "            for name, param in get_chained_attr(self.blackbox_model, finetune_layer).named_parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a7b754-b6c4-4306-b875-5aca99977593",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SOP Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ece59ab-5084-416e-b688-9b884b7a9556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SOPImage(SOP):\n",
    "    def __init__(self, \n",
    "                 config,\n",
    "                 blackbox_model,\n",
    "                 class_weights,\n",
    "                 projection_layer=None,\n",
    "                 ):\n",
    "        super().__init__(config,\n",
    "                            blackbox_model,\n",
    "                            class_weights\n",
    "                            )\n",
    "        \n",
    "        self.image_size = config.image_size if isinstance(config.image_size, \n",
    "                                                    collections.abc.Iterable) \\\n",
    "                                            else (config.image_size, config.image_size)\n",
    "        self.num_channels = config.num_channels\n",
    "        # attention args\n",
    "        self.attn_patch_size = config.attn_patch_size\n",
    "\n",
    "        if projection_layer is not None:\n",
    "            self.projection = copy.deepcopy(projection_layer)\n",
    "        else:\n",
    "            self.init_projection()\n",
    "\n",
    "        # Initialize the weights of the model\n",
    "        # self.init_weights()\n",
    "        self.init_grads()\n",
    "\n",
    "    def init_projection(self):\n",
    "        self.projection = nn.Conv2d(self.config.num_channels, \n",
    "                                    self.hidden_size, \n",
    "                                    kernel_size=self.attn_patch_size, \n",
    "                                    stride=self.attn_patch_size)  # make each patch a vec\n",
    "        self.projection_up = nn.ConvTranspose2d(1, \n",
    "                                                    1, \n",
    "                                                    kernel_size=self.attn_patch_size, \n",
    "                                                    stride=self.attn_patch_size)  # make each patch a vec\n",
    "        self.projection_up.weight = nn.Parameter(torch.ones_like(self.projection_up.weight))\n",
    "        self.projection_up.bias = nn.Parameter(torch.zeros_like(self.projection_up.bias))\n",
    "        self.projection_up.weight.requires_grad = False\n",
    "        self.projection_up.bias.requires_grad = False\n",
    "\n",
    "    def forward(self, \n",
    "                inputs, \n",
    "                segs=None, \n",
    "                input_mask_weights=None,\n",
    "                epoch=-1, \n",
    "                mask_batch_size=16,\n",
    "                label=None,\n",
    "                return_tuple=False):\n",
    "        if epoch == -1:\n",
    "            epoch = self.num_heads\n",
    "        bsz, num_channel, img_dim1, img_dim2 = inputs.shape\n",
    "        \n",
    "        # Mask (Group) generation\n",
    "        if input_mask_weights is None:\n",
    "            grouped_inputs, input_mask_weights = self.group_generate(inputs, epoch, mask_batch_size, segs)\n",
    "        else:\n",
    "            grouped_inputs = inputs.unsqueeze(1) * input_mask_weights.unsqueeze(2) # directly apply mask\n",
    "        \n",
    "        # Backbone model\n",
    "        logits, pooler_outputs = self.run_backbone(grouped_inputs, mask_batch_size)\n",
    "        # return logits\n",
    "\n",
    "        # Mask (Group) selection & aggregation\n",
    "        # return self.group_select(logits, pooler_outputs, img_dim1, img_dim2)\n",
    "        weighted_logits, output_mask_weights, logits, pooler_outputs = self.group_select(logits, pooler_outputs, img_dim1, img_dim2)\n",
    "\n",
    "        if return_tuple:\n",
    "            return self.get_results_tuple(weighted_logits, logits, pooler_outputs, input_mask_weights, output_mask_weights, bsz, label)\n",
    "        else:\n",
    "            return weighted_logits\n",
    "\n",
    "    def get_results_tuple(self, weighted_logits, logits, pooler_outputs, input_mask_weights, output_mask_weights, bsz, label):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run_backbone(self, masked_inputs, mask_batch_size):\n",
    "        bsz, num_masks, num_channel, img_dim1, img_dim2 = masked_inputs.shape\n",
    "        masked_inputs = masked_inputs.view(-1, num_channel, img_dim1, img_dim2)\n",
    "        logits = []\n",
    "        pooler_outputs = []\n",
    "        for i in range(0, masked_inputs.shape[0], mask_batch_size):\n",
    "            output_i = self.blackbox_model(\n",
    "                masked_inputs[i:i+mask_batch_size]\n",
    "            )\n",
    "            pooler_i = output_i.pooler_output\n",
    "            logits_i = output_i.logits\n",
    "            logits.append(logits_i)\n",
    "            pooler_outputs.append(pooler_i)\n",
    "\n",
    "        logits = torch.cat(logits).view(bsz, num_masks, self.num_classes, -1)\n",
    "        pooler_outputs = torch.cat(pooler_outputs).view(bsz, num_masks, self.hidden_size, -1)\n",
    "        return logits, pooler_outputs\n",
    "    \n",
    "    def group_generate(self, inputs, epoch, mask_batch_size, segs=None):\n",
    "        bsz, num_channel, img_dim1, img_dim2 = inputs.shape\n",
    "        if segs is None:   # should be renamed \"segments\"\n",
    "            projected_inputs = self.projection(inputs)\n",
    "            projected_inputs = projected_inputs.flatten(2).transpose(1, 2)  # bsz, img_dim1 * img_dim2, num_channel\n",
    "            projected_inputs = projected_inputs * self.projected_input_scale\n",
    "\n",
    "            if self.num_masks_max != -1:\n",
    "                input_dropout_idxs = torch.randperm(projected_inputs.shape[1])[:self.num_masks_max]\n",
    "                projected_query = projected_inputs[:, input_dropout_idxs]\n",
    "            else:\n",
    "                projected_query = projected_inputs\n",
    "            input_mask_weights_cand = self.input_attn(projected_query, projected_inputs, epoch=epoch)\n",
    "            \n",
    "            num_patches = ((self.image_size[0] - self.attn_patch_size) // self.attn_patch_size + 1, \n",
    "                        (self.image_size[1] - self.attn_patch_size) // self.attn_patch_size + 1)\n",
    "            input_mask_weights_cand = input_mask_weights_cand.reshape(-1, 1, num_patches[0], num_patches[1])\n",
    "            input_mask_weights_cand = self.projection_up(input_mask_weights_cand, \n",
    "                                                         output_size=torch.Size([input_mask_weights_cand.shape[0], 1, \n",
    "                                                                                 img_dim1, img_dim2]))\n",
    "            input_mask_weights_cand = input_mask_weights_cand.view(bsz, -1, img_dim1, img_dim2)\n",
    "            input_mask_weights_cand = torch.clip(input_mask_weights_cand, max=1.0)\n",
    "        else:\n",
    "            # With/without masks are a bit different. Should we make them the same? Need to experiment.\n",
    "            bsz, num_segs, img_dim1, img_dim2 = segs.shape\n",
    "            seged_output_0 = inputs.unsqueeze(1) * segs.unsqueeze(2) # (bsz, num_masks, num_channel, img_dim1, img_dim2)\n",
    "            interm_outputs, _ = self.run_backbone(seged_output_0, mask_batch_size)\n",
    "            \n",
    "            interm_outputs = interm_outputs.view(bsz, -1, self.hidden_size)\n",
    "            interm_outputs = interm_outputs * self.projected_input_scale\n",
    "            segment_mask_weights = self.input_attn(interm_outputs, interm_outputs, epoch=epoch)\n",
    "            segment_mask_weights = segment_mask_weights.reshape(bsz, -1, num_segs)\n",
    "            \n",
    "            new_masks =  segs.unsqueeze(1) * segment_mask_weights.unsqueeze(-1).unsqueeze(-1)\n",
    "            # (bsz, num_new_masks, num_masks, img_dim1, img_dim2)\n",
    "            input_mask_weights_cand = new_masks.sum(2)  # if one mask has it, then have it\n",
    "            # todo: Can we simplify the above to be dot product?\n",
    "            \n",
    "        scale_factor = 1.0 / input_mask_weights_cand.reshape(bsz, -1, \n",
    "                                                        img_dim1 * img_dim2).max(dim=-1).values\n",
    "        input_mask_weights_cand = input_mask_weights_cand * scale_factor.view(bsz, -1,1,1)\n",
    "        \n",
    "        \n",
    "        # we are using iterative training\n",
    "        # we will train some masks every epoch\n",
    "        # the masks to train are selected by mod of epoch number\n",
    "        # Dropout for training\n",
    "        if self.training:\n",
    "            dropout_idxs = torch.randperm(input_mask_weights_cand.shape[1])[:self.num_masks_sample]\n",
    "            dropout_mask = torch.zeros(bsz, input_mask_weights_cand.shape[1]).to(inputs.device)\n",
    "            dropout_mask[:,dropout_idxs] = 1\n",
    "        else:\n",
    "            dropout_mask = torch.ones(bsz, input_mask_weights_cand.shape[1]).to(inputs.device)\n",
    "        \n",
    "        input_mask_weights = input_mask_weights_cand[dropout_mask.bool()].clone()\n",
    "        input_mask_weights = input_mask_weights.view(bsz, -1, img_dim1, img_dim2)\n",
    "\n",
    "        masked_inputs = inputs.unsqueeze(1) * input_mask_weights.unsqueeze(2)\n",
    "        return masked_inputs, input_mask_weights\n",
    "    \n",
    "    def group_select(self, logits, pooler_outputs, img_dim1, img_dim2):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc85583-216c-4d21-b5cf-23e5767b3239",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SOPImageCls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "290bd64c-8939-44de-94a2-c9f647762e09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SOPImageCls(SOPImage):\n",
    "    def group_select(self, logits, pooler_outputs, img_dim1, img_dim2):\n",
    "        bsz, num_masks = logits.shape[:2]\n",
    "\n",
    "        logits = logits.view(bsz, num_masks, self.num_classes)\n",
    "        pooler_outputs = pooler_outputs.view(bsz, num_masks, self.hidden_size)\n",
    "\n",
    "        query = self.class_weights.unsqueeze(0).expand(bsz, \n",
    "                                                    self.num_classes, \n",
    "                                                    self.hidden_size) #.to(logits.device)\n",
    "        \n",
    "        key = pooler_outputs\n",
    "        weighted_logits, output_mask_weights = self.output_attn(query, key, logits)\n",
    "\n",
    "        return weighted_logits, output_mask_weights, logits, pooler_outputs\n",
    "    \n",
    "    def get_results_tuple(self, weighted_logits, logits, pooler_outputs, input_mask_weights, output_mask_weights, bsz, label):\n",
    "        # todo: debug for segmentation\n",
    "        masks_aggr = None\n",
    "        masks_aggr_pred_cls = None\n",
    "        masks_max_pred_cls = None\n",
    "        flat_masks = None\n",
    "\n",
    "        if label is not None:\n",
    "            predicted = label  # allow labels to be different\n",
    "        else:\n",
    "            _, predicted = torch.max(weighted_logits.data, -1)\n",
    "        \n",
    "        masks_mult = input_mask_weights.unsqueeze(2) * \\\n",
    "        output_mask_weights.unsqueeze(-1).unsqueeze(-1) # bsz, n_masks, n_cls, img_dim, img_dim\n",
    "        \n",
    "        masks_aggr = masks_mult.sum(1) # bsz, n_cls, img_dim, img_dim OR bsz, n_cls, seq_len\n",
    "        masks_aggr_pred_cls = masks_aggr[range(bsz), predicted].unsqueeze(1)\n",
    "        max_mask_indices = output_mask_weights.max(2).values.max(1).indices\n",
    "        masks_max_pred_cls = masks_mult[range(bsz),max_mask_indices,predicted].unsqueeze(1)\n",
    "        flat_masks = compress_masks_image(input_mask_weights, output_mask_weights)\n",
    "        return AttributionOutputSOP(weighted_logits,\n",
    "                                    logits,\n",
    "                                    pooler_outputs,\n",
    "                                    input_mask_weights,\n",
    "                                    output_mask_weights,\n",
    "                                    masks_aggr_pred_cls,\n",
    "                                    masks_max_pred_cls,\n",
    "                                    masks_aggr,\n",
    "                                    flat_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa8035d-bb81-4b20-b2af-5606a899e3c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SOPImageSeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c8d2da4-378c-4c80-ab43-46b3b0a7dec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SOPImageSeg(SOPImage):\n",
    "    def group_select(self, logits, pooler_outputs, img_dim1, img_dim2):\n",
    "        bsz, num_masks = logits.shape[:2]\n",
    "        logits = logits.view(bsz, num_masks, self.num_classes, \n",
    "                                            img_dim1, img_dim2)\n",
    "        pooler_outputs = pooler_outputs.view(bsz, num_masks, \n",
    "                                                        self.hidden_size, \n",
    "                                                        img_dim1, \n",
    "                                                        img_dim2)\n",
    "        # return pooler_outputs\n",
    "        query = self.class_weights.unsqueeze(0) \\\n",
    "            .view(1, self.num_classes, self.hidden_size, -1).mean(-1) \\\n",
    "            .expand(bsz, self.num_classes, self.hidden_size).to(logits.device)\n",
    "        pooler_outputs.requires_grad = True\n",
    "        key = pooler_outputs.view(bsz, num_masks, self.hidden_size, -1).mean(-1)\n",
    "        weighted_logits, output_mask_weights = self.output_attn(query, key, logits)\n",
    "\n",
    "        return weighted_logits, output_mask_weights, logits, pooler_outputs\n",
    "    \n",
    "    def get_results_tuple(self, weighted_logits, logits, pooler_outputs, input_mask_weights, output_mask_weights, bsz, label):\n",
    "        # todo: debug for segmentation\n",
    "        masks_aggr = None\n",
    "        masks_aggr_pred_cls = None\n",
    "        masks_max_pred_cls = None\n",
    "        flat_masks = None\n",
    "\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        # _, predicted = torch.max(weighted_output.data, -1)\n",
    "        masks_mult = input_mask_weights.unsqueeze(2) * output_mask_weights.unsqueeze(-1).unsqueeze(-1) # bsz, n_masks, n_cls, img_dim, img_dim\n",
    "        masks_aggr = masks_mult.sum(1) # bsz, n_cls, img_dim, img_dim OR bsz, n_cls, seq_len\n",
    "        # masks_aggr_pred_cls = masks_aggr\n",
    "        # masks_aggr_pred_cls = masks_aggr[range(bsz), predicted].unsqueeze(1)\n",
    "        # max_mask_indices = output_mask_weights.argmax(1)\n",
    "        # masks_max_pred_cls = masks_mult[max_mask_indices[:,0]]\n",
    "        # masks_max_pred_cls = max_mask_indices\n",
    "        # TODO: this has some problems ^\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        \n",
    "        flat_masks = compress_masks_image(input_mask_weights, output_mask_weights)\n",
    "\n",
    "        return AttributionOutputSOP(weighted_logits,\n",
    "                                    logits,\n",
    "                                    pooler_outputs,\n",
    "                                    input_mask_weights,\n",
    "                                    output_mask_weights,\n",
    "                                    masks_aggr_pred_cls,\n",
    "                                    masks_max_pred_cls,\n",
    "                                    masks_aggr,\n",
    "                                    flat_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485bee55-3e7b-47f8-8676-5c525451c119",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e7838f6-1134-4508-ab0c-63e9b48b99cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, AutoConfig\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import sys\n",
    "sys.path.append('../lib/exlib/src')\n",
    "# from exlib.modules.sop import SOPImageCls, SOPConfig, get_chained_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebdbef0b-ef0b-403b-97f2-24c9b67fdaa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "SEED = 42\n",
    "if SEED != -1:\n",
    "    # Torch RNG\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    # Python RNG\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c936663e-2c91-49af-a263-30ffbeeda7d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model paths\n",
    "backbone_model_name = 'farleyknight-org-username/vit-base-mnist'\n",
    "backbone_processor_name = 'farleyknight-org-username/vit-base-mnist'\n",
    "# sop_config_path = 'configs/imagenet_m.json'\n",
    "\n",
    "# data paths\n",
    "# TRAIN_DATA_DIR = '../data/imagenet_m/train'\n",
    "# VAL_DATA_DIR = '../data/imagenet_m/val'\n",
    "\n",
    "# training args\n",
    "batch_size = 5\n",
    "lr = 0.000005\n",
    "num_epochs = 20\n",
    "warmup_steps = 20  # 2000\n",
    "mask_batch_size = 64\n",
    "\n",
    "# experiment args\n",
    "exp_dir = '../exps/imagenet_m'\n",
    "os.makedirs(exp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a3e71e3-0fef-4072-a32f-12097a830cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "\n",
    "backbone_model = AutoModelForImageClassification.from_pretrained(backbone_model_name)\n",
    "processor = ViTFeatureExtractor.from_pretrained(backbone_processor_name)\n",
    "backbone_config = AutoConfig.from_pretrained(backbone_model_name)\n",
    "\n",
    "config = SOPConfig(\n",
    "    attn_patch_size=16,\n",
    "    num_heads=1,\n",
    "    num_masks_sample=20,\n",
    "    num_masks_max=200,\n",
    "    finetune_layers=['model.classifier']\n",
    ")\n",
    "config.__dict__.update(backbone_config.__dict__)\n",
    "config.num_labels = len(backbone_config.label2id)\n",
    "# config.save_pretrained(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aef0835b-02b8-40b1-bac9-35a92d9cde77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchvision.transforms import ToPILImage\n",
    "from datasets import load_dataset\n",
    "\n",
    "def transform(example):\n",
    "    # Convert the image data to a PIL Image\n",
    "    image = example['image']\n",
    "    \n",
    "    # Preprocess the image using the ViTImageProcessor\n",
    "    image = image.convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors='pt')\n",
    "    # import pdb; pdb.set_trace()\n",
    "    return {'pixel_values': inputs['pixel_values'].squeeze(0), 'label': torch.tensor(example['label'])}\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Assuming batch is a list of dictionaries\n",
    "    # import pdb; pdb.set_trace()\n",
    "    pixel_values = torch.tensor([item['pixel_values'] for item in batch])\n",
    "    labels = torch.tensor([item['label'] for item in batch])\n",
    "\n",
    "    return {'pixel_values': pixel_values, 'label': labels}\n",
    "\n",
    "# Load the dataset\n",
    "# train_dataset = ImageFolder(root=TRAIN_DATA_DIR, transform=transform)\n",
    "# val_dataset = ImageFolder(root=VAL_DATA_DIR, transform=transform)\n",
    "train_dataset = load_dataset('mnist', split='train[:100]')\n",
    "val_dataset = load_dataset('mnist', split='test[:100]')\n",
    "\n",
    "# Use subset for testing purpose\n",
    "# num_data = 100\n",
    "# train_dataset = Subset(train_dataset, range(num_data))\n",
    "# val_dataset = Subset(val_dataset, range(num_data))\n",
    "\n",
    "# Apply the transformation\n",
    "train_dataset = train_dataset.map(lambda examples: transform(examples), batched=False, remove_columns=[\"image\"])\n",
    "val_dataset = val_dataset.map(lambda examples: transform(examples), batched=False, remove_columns=[\"image\"])\n",
    "\n",
    "# Create a DataLoader to batch and shuffle the data\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cea2f26-b9e6-4abe-83bd-5050dcbb16f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "WrappedBackboneOutput = namedtuple(\"WrappedBackboneOutput\", \n",
    "                                  [\"logits\",\n",
    "                                   \"pooler_output\"])\n",
    "\n",
    "\n",
    "class WrappedBackboneModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(inputs, output_hidden_states=True)\n",
    "        return WrappedBackboneOutput(outputs.logits, outputs.hidden_states[-1][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e4b54a6-80d5-4bfd-ad56-39a862f28d09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wrapped_backbone_model = WrappedBackboneModel(backbone_model)\n",
    "wrapped_backbone_model = wrapped_backbone_model.to(device)\n",
    "class_weights = get_chained_attr(wrapped_backbone_model, config.finetune_layers[0]).weight.clone() #.to(device)\n",
    "# class_weights.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0bf3ed2f-bae5-4a0a-8481-392821c34cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shallow copy class weights\n"
     ]
    }
   ],
   "source": [
    "model = SOPImageCls(config, wrapped_backbone_model, class_weights=class_weights, projection_layer=None)\n",
    "# model = backbone_model\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d698838c-d435-4bad-b200-9c0d0a4701d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "            'inverse_sqrt',\n",
    "            optimizer=optimizer, \n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4128a754-4503-41c2-8970-a1d432844876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval(model, dataloader, criterion):\n",
    "    print('Eval ...')\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        progress_bar_eval = tqdm(range(len(dataloader)))\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # Now you can use `inputs` and `labels` in your training loop.\n",
    "            inputs, labels = batch['pixel_values'], batch['label']\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            \n",
    "            # val loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # acc\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            \n",
    "            progress_bar_eval.update(1)\n",
    "    \n",
    "    val_acc = correct / total\n",
    "    val_loss = total_loss / total\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return {\n",
    "        'val_acc': val_acc,\n",
    "        'val_loss': val_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "681e0482-03c0-4d9c-84ef-5d7c783f9e99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9d689e4-bf44-4130-84db-bec9aeac8cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values', 'label'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a24390d-4e6b-4286-b412-8ba620b35953",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, torch.Tensor, 224, torch.Tensor)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch['pixel_values'][0]), type(batch['pixel_values'][0][0]), len(batch['pixel_values'][0][0]), type(batch['pixel_values'][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae07940a-296b-478b-bc20-691898613b40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d023f8b09614c4ba7dd9658e1e8fe9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 20, Loss 1.7502, LR 0.00000475\n",
      "Eval ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5801b1fe6c894f78affdd82000f2d47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 19, Val acc 0.6400, Val loss 0.2299\n",
      "Best checkpoint saved at ../exps/imagenet_m/best/model.pth\n",
      "Last checkpoint saved at ../exps/imagenet_m/last/model.pth\n",
      "Epoch 2, Batch 20, Loss 1.1286, LR 0.00000358\n",
      "Eval ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51202a85cc074066a18de7c6b4203266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 39, Val acc 0.7700, Val loss 0.1638\n",
      "Best checkpoint saved at ../exps/imagenet_m/best/model.pth\n",
      "Last checkpoint saved at ../exps/imagenet_m/last/model.pth\n",
      "Epoch 3, Batch 20, Loss 0.8466, LR 0.00000291\n",
      "Eval ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd54b66e15aa4c30ab549a4793a7ae5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 59, Val acc 0.8000, Val loss 0.1371\n",
      "Best checkpoint saved at ../exps/imagenet_m/best/model.pth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2802/3988360367.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mlast_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Last checkpoint saved at {last_checkpoint_path}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "def check_gradients(model):\n",
    "    zero_gradients = True\n",
    "    no_gradients = True\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if param.grad is not None:\n",
    "                print(name + ' has grad')\n",
    "                no_gradients = False\n",
    "                if param.grad.data.abs().sum() > 0:\n",
    "                    zero_gradients = False\n",
    "                    # break\n",
    "\n",
    "    if no_gradients:\n",
    "        print(\"Gradients are not computed for any parameter.\")\n",
    "    elif zero_gradients:\n",
    "        print(\"Gradients are zero for all computed parameters.\")\n",
    "    else:\n",
    "        print(\"Gradients are flowing.\")\n",
    "\n",
    "def print_grad(grad):\n",
    "    print(grad)\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    output.register_hook(print_grad)\n",
    "\n",
    "hook = model.output_attn.multihead_attn.out_proj.register_forward_hook(hook_fn)\n",
    "\n",
    "\n",
    "# track = True\n",
    "track = False\n",
    "\n",
    "if track:\n",
    "    import wandb\n",
    "    wandb.init(project='sop')\n",
    "    wandb.run.name = os.path.basename(exp_dir)\n",
    "\n",
    "# Iterate over the data\n",
    "best_val_acc = 0.0\n",
    "step = 0\n",
    "\n",
    "logging.basicConfig(filename=os.path.join(exp_dir, 'train.log'), level=logging.INFO)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_total = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        inputs, labels = batch['pixel_values'], batch['label']\n",
    "        # inputs = torch.tensor(inputs)\n",
    "        # labels = torch.tensor(labels)\n",
    "        # print(len(inputs), len(labels))\n",
    "        # print(inputs[0].shape)\n",
    "        # print(labels[0])\n",
    "        # print(batch)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # print('inputs', inputs.shape)\n",
    "        # print('labels', labels.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs, mask_batch_size=mask_batch_size)\n",
    "        \n",
    "        def print_grad(grad):\n",
    "            pass\n",
    "            # print('hi', grad)\n",
    "\n",
    "        logits.register_hook(print_grad)\n",
    "        def hook_fn(module, input, output):\n",
    "            # import pdb; pdb.set_trace()\n",
    "            output.register_hook(print_grad)\n",
    "\n",
    "        hook = model.output_attn.multihead_attn.out_proj.register_forward_hook(hook_fn)\n",
    "        \n",
    "        # for name, module in model.named_children():\n",
    "        #     module.register_forward_hook(hook_fn)\n",
    "\n",
    "\n",
    "        # logits = model(inputs).logits\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        # check_gradients(model)\n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        running_total += labels.size(0)\n",
    "        \n",
    "        if i % 100 == 99 or i == len(train_dataloader) - 1:\n",
    "            # Print training loss every 100 batches\n",
    "            curr_lr = float(optimizer.param_groups[0]['lr'])\n",
    "            log_message = f'Epoch {epoch + 1}, Batch {i + 1}, Loss {running_loss / running_total:.4f}, LR {curr_lr:.8f}'\n",
    "            print(log_message)\n",
    "            logging.info(log_message)\n",
    "            if track:\n",
    "                wandb.log({'train_loss': running_loss / running_total,\n",
    "                        'lr': curr_lr,\n",
    "                        'epoch': epoch,\n",
    "                        'step': step})\n",
    "            running_loss = 0.0\n",
    "            running_total = 0\n",
    "            \n",
    "        if i % 1000 == 999 or i == len(train_dataloader) - 1:\n",
    "            val_results = eval(model, val_dataloader, criterion)\n",
    "            val_acc = val_results['val_acc']\n",
    "            val_loss = val_results['val_loss']\n",
    "            log_message = f'Epoch {epoch}, Step {step}, Val acc {val_acc:.4f}, Val loss {val_loss:.4f}'\n",
    "            print(log_message)\n",
    "            logging.info(log_message)\n",
    "            if track:\n",
    "                wandb.log({'val_acc': val_acc,\n",
    "                           'val_loss': val_loss,\n",
    "                        'epoch': epoch,\n",
    "                        'step': step})\n",
    "            \n",
    "            last_dir = os.path.join(exp_dir, 'last')\n",
    "            best_dir = os.path.join(exp_dir, 'best')\n",
    "            checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'step': step,\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_acc': val_acc,\n",
    "                }\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                os.makedirs(best_dir, exist_ok=True)\n",
    "                best_checkpoint_path = os.path.join(best_dir, 'model.pth')\n",
    "                torch.save(checkpoint, best_checkpoint_path)\n",
    "                print(f'Best checkpoint saved at {best_checkpoint_path}')\n",
    "                # model.save_pretrained(best_dir)\n",
    "            # model.save_pretrained(last_dir)\n",
    "            os.makedirs(last_dir, exist_ok=True)\n",
    "            last_checkpoint_path = os.path.join(last_dir, 'model.pth')\n",
    "            torch.save(checkpoint, last_checkpoint_path)\n",
    "            print(f'Last checkpoint saved at {last_checkpoint_path}')\n",
    "            \n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4de143-8424-4b06-82d3-1bb29ec9366c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad8acb-8c96-4951-8fb2-117738cf971d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
