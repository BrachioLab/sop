{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e7838f6-1134-4508-ab0c-63e9b48b99cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, AutoConfig\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import sys\n",
    "sys.path.append('../lib/exlib/src')\n",
    "from exlib.modules.sop import SOPImageCls, SOPConfig, get_chained_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebdbef0b-ef0b-403b-97f2-24c9b67fdaa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "SEED = 42\n",
    "if SEED != -1:\n",
    "    # Torch RNG\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    # Python RNG\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c936663e-2c91-49af-a263-30ffbeeda7d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model paths\n",
    "backbone_model_name = '../pt_models/vit-base-patch16-224-imagenet10cls'\n",
    "backbone_processor_name = 'google/vit-base-patch16-224'\n",
    "# sop_config_path = 'configs/imagenet_m.json'\n",
    "\n",
    "# data paths\n",
    "TRAIN_DATA_DIR = '../data/imagenet_m/train'\n",
    "VAL_DATA_DIR = '../data/imagenet_m/val'\n",
    "\n",
    "# training args\n",
    "batch_size = 16\n",
    "lr = 0.000005\n",
    "num_epochs = 20\n",
    "warmup_steps = 2000\n",
    "mask_batch_size = 64\n",
    "\n",
    "# experiment args\n",
    "exp_dir = '../exps/imagenet_m'\n",
    "os.makedirs(exp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a3e71e3-0fef-4072-a32f-12097a830cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backbone_model = AutoModelForImageClassification.from_pretrained(backbone_model_name)\n",
    "processor = AutoImageProcessor.from_pretrained(backbone_processor_name)\n",
    "backbone_config = AutoConfig.from_pretrained(backbone_model_name)\n",
    "\n",
    "config = SOPConfig(\n",
    "    attn_patch_size=16,\n",
    "    num_heads=1,\n",
    "    num_masks_sample=20,\n",
    "    num_masks_max=200,\n",
    "    finetune_layers=['model.classifier']\n",
    ")\n",
    "config.__dict__.update(backbone_config.__dict__)\n",
    "config.num_labels = len(backbone_config.label2id)\n",
    "# config.save_pretrained(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aef0835b-02b8-40b1-bac9-35a92d9cde77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def transform(image):\n",
    "    # Preprocess the image using the ViTImageProcessor\n",
    "    image = image.convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors='pt')\n",
    "    return inputs['pixel_values'].squeeze(0)\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = ImageFolder(root=TRAIN_DATA_DIR, transform=transform)\n",
    "val_dataset = ImageFolder(root=VAL_DATA_DIR, transform=transform)\n",
    "\n",
    "# Use subset for testing purpose\n",
    "# num_data = 100\n",
    "# train_dataset = Subset(train_dataset, range(num_data))\n",
    "# val_dataset = Subset(val_dataset, range(num_data))\n",
    "\n",
    "# Create a DataLoader to batch and shuffle the data\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cea2f26-b9e6-4abe-83bd-5050dcbb16f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "WrappedBackboneOutput = namedtuple(\"WrappedBackboneOutput\", \n",
    "                                  [\"logits\",\n",
    "                                   \"pooler_output\"])\n",
    "\n",
    "\n",
    "class WrappedBackboneModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(inputs, output_hidden_states=True)\n",
    "        return WrappedBackboneOutput(outputs.logits, outputs.hidden_states[-1][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e4b54a6-80d5-4bfd-ad56-39a862f28d09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wrapped_backbone_model = WrappedBackboneModel(backbone_model)\n",
    "wrapped_backbone_model = wrapped_backbone_model.to(device)\n",
    "class_weights = get_chained_attr(wrapped_backbone_model, config.finetune_layers[0]).weight #.clone().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bf3ed2f-bae5-4a0a-8481-392821c34cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep copy class weights\n"
     ]
    }
   ],
   "source": [
    "model = SOPImageCls(config, wrapped_backbone_model, class_weights=class_weights, projection_layer=None)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d698838c-d435-4bad-b200-9c0d0a4701d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "            'inverse_sqrt',\n",
    "            optimizer=optimizer, \n",
    "            num_warmup_steps=warmup_steps\n",
    "        )\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4128a754-4503-41c2-8970-a1d432844876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval(model, dataloader, criterion):\n",
    "    print('Eval ...')\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        progress_bar_eval = tqdm(range(len(dataloader)))\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # Now you can use `inputs` and `labels` in your training loop.\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            \n",
    "            # val loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # acc\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            \n",
    "            progress_bar_eval.update(1)\n",
    "    \n",
    "    val_acc = correct / total\n",
    "    val_loss = total_loss / total\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return {\n",
    "        'val_acc': val_acc,\n",
    "        'val_loss': val_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae07940a-296b-478b-bc20-691898613b40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfallcat\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/shared_data0/weiqiuy/sop/notebooks/wandb/run-20231031_150801-lyxyfjg6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fallcat/sop/runs/lyxyfjg6' target=\"_blank\">headless-vampire-8</a></strong> to <a href='https://wandb.ai/fallcat/sop' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fallcat/sop' target=\"_blank\">https://wandb.ai/fallcat/sop</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fallcat/sop/runs/lyxyfjg6' target=\"_blank\">https://wandb.ai/fallcat/sop/runs/lyxyfjg6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0df24eed1a47bea2b5c9014983fe45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 100, Loss 1.0559, LR 0.00000025\n",
      "Epoch 0, Batch 200, Loss 0.9883, LR 0.00000050\n",
      "Epoch 0, Batch 300, Loss 0.7779, LR 0.00000075\n",
      "Epoch 0, Batch 400, Loss 0.7004, LR 0.00000100\n",
      "Epoch 0, Batch 500, Loss 0.6542, LR 0.00000125\n",
      "Epoch 0, Batch 600, Loss 0.5694, LR 0.00000150\n",
      "Epoch 0, Batch 700, Loss 0.5254, LR 0.00000175\n",
      "Epoch 0, Batch 800, Loss 0.5095, LR 0.00000200\n",
      "Epoch 0, Batch 813, Loss 0.4667, LR 0.00000203\n",
      "Eval ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9b4d8c22874a2fb72a781f877e68e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 812, Val acc 0.8060, Val loss 0.0336\n",
      "Best checkpoint saved at ../exps/imagenet_m/best/checkpoint.pth\n",
      "Last checkpoint saved at ../exps/imagenet_m/last/checkpoint.pth\n",
      "Epoch 1, Batch 100, Loss 0.4341, LR 0.00000228\n",
      "Epoch 1, Batch 200, Loss 0.4446, LR 0.00000253\n",
      "Epoch 1, Batch 300, Loss 0.3710, LR 0.00000278\n",
      "Epoch 1, Batch 400, Loss 0.3719, LR 0.00000303\n",
      "Epoch 1, Batch 500, Loss 0.3902, LR 0.00000328\n",
      "Epoch 1, Batch 600, Loss 0.4610, LR 0.00000353\n",
      "Epoch 1, Batch 700, Loss 0.3910, LR 0.00000378\n",
      "Epoch 1, Batch 800, Loss 0.4060, LR 0.00000403\n",
      "Epoch 1, Batch 813, Loss 0.3106, LR 0.00000406\n",
      "Eval ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0056282d33294e14a19b6b73b74ff157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 1625, Val acc 0.8560, Val loss 0.0271\n",
      "Best checkpoint saved at ../exps/imagenet_m/best/checkpoint.pth\n",
      "Last checkpoint saved at ../exps/imagenet_m/last/checkpoint.pth\n",
      "Epoch 2, Batch 100, Loss 0.3843, LR 0.00000431\n",
      "Epoch 2, Batch 200, Loss 0.3632, LR 0.00000456\n",
      "Epoch 2, Batch 300, Loss 0.3719, LR 0.00000481\n",
      "Epoch 2, Batch 400, Loss 0.4001, LR 0.00000497\n",
      "Epoch 2, Batch 500, Loss 0.4232, LR 0.00000485\n",
      "Epoch 2, Batch 600, Loss 0.3527, LR 0.00000474\n",
      "Epoch 2, Batch 700, Loss 0.3636, LR 0.00000464\n",
      "Epoch 2, Batch 800, Loss 0.3434, LR 0.00000454\n",
      "Epoch 2, Batch 813, Loss 0.4273, LR 0.00000453\n",
      "Eval ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b1351084ec4fa1bbfdb1ab10d28e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 2438, Val acc 0.8680, Val loss 0.0263\n",
      "Best checkpoint saved at ../exps/imagenet_m/best/checkpoint.pth\n",
      "Last checkpoint saved at ../exps/imagenet_m/last/checkpoint.pth\n",
      "Epoch 3, Batch 100, Loss 0.3837, LR 0.00000444\n",
      "Epoch 3, Batch 200, Loss 0.3694, LR 0.00000435\n",
      "Epoch 3, Batch 300, Loss 0.3461, LR 0.00000427\n",
      "Epoch 3, Batch 400, Loss 0.3209, LR 0.00000420\n",
      "Epoch 3, Batch 500, Loss 0.3450, LR 0.00000413\n",
      "Epoch 3, Batch 600, Loss 0.3247, LR 0.00000406\n",
      "Epoch 3, Batch 700, Loss 0.3175, LR 0.00000399\n",
      "Epoch 3, Batch 800, Loss 0.3763, LR 0.00000393\n",
      "Epoch 3, Batch 813, Loss 0.3376, LR 0.00000392\n",
      "Eval ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164a25dfb81b4fd8ac27e023a70ef797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 3251, Val acc 0.8760, Val loss 0.0215\n",
      "Best checkpoint saved at ../exps/imagenet_m/best/checkpoint.pth\n",
      "Last checkpoint saved at ../exps/imagenet_m/last/checkpoint.pth\n",
      "Epoch 4, Batch 100, Loss 0.3173, LR 0.00000386\n",
      "Epoch 4, Batch 200, Loss 0.3370, LR 0.00000381\n",
      "Epoch 4, Batch 300, Loss 0.3347, LR 0.00000375\n",
      "Epoch 4, Batch 400, Loss 0.3429, LR 0.00000370\n",
      "Epoch 4, Batch 500, Loss 0.3559, LR 0.00000365\n",
      "Epoch 4, Batch 600, Loss 0.3749, LR 0.00000360\n",
      "Epoch 4, Batch 700, Loss 0.3441, LR 0.00000356\n",
      "Epoch 4, Batch 800, Loss 0.2651, LR 0.00000351\n",
      "Epoch 4, Batch 813, Loss 0.2991, LR 0.00000351\n",
      "Eval ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4802f5fe350d4ed5ae75bb5f28db6102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 4064, Val acc 0.8900, Val loss 0.0211\n",
      "Best checkpoint saved at ../exps/imagenet_m/best/checkpoint.pth\n",
      "Last checkpoint saved at ../exps/imagenet_m/last/checkpoint.pth\n",
      "Epoch 5, Batch 100, Loss 0.3020, LR 0.00000347\n",
      "Epoch 5, Batch 200, Loss 0.3595, LR 0.00000342\n",
      "Epoch 5, Batch 300, Loss 0.2984, LR 0.00000338\n",
      "Epoch 5, Batch 400, Loss 0.3193, LR 0.00000335\n",
      "Epoch 5, Batch 500, Loss 0.3058, LR 0.00000331\n",
      "Epoch 5, Batch 600, Loss 0.2986, LR 0.00000327\n",
      "Epoch 5, Batch 700, Loss 0.2836, LR 0.00000324\n",
      "Epoch 5, Batch 800, Loss 0.3101, LR 0.00000321\n",
      "Epoch 5, Batch 813, Loss 0.3845, LR 0.00000320\n",
      "Eval ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f9e24963a64d4c92440fa844d9d3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 4877, Val acc 0.8860, Val loss 0.0216\n",
      "Last checkpoint saved at ../exps/imagenet_m/last/checkpoint.pth\n",
      "Epoch 6, Batch 100, Loss 0.3318, LR 0.00000317\n",
      "Epoch 6, Batch 200, Loss 0.3022, LR 0.00000314\n",
      "Epoch 6, Batch 300, Loss 0.2911, LR 0.00000311\n",
      "Epoch 6, Batch 400, Loss 0.3251, LR 0.00000308\n",
      "Epoch 6, Batch 500, Loss 0.3230, LR 0.00000305\n",
      "Epoch 6, Batch 600, Loss 0.3608, LR 0.00000302\n",
      "Epoch 6, Batch 700, Loss 0.3706, LR 0.00000299\n",
      "Epoch 6, Batch 800, Loss 0.3164, LR 0.00000297\n",
      "Epoch 6, Batch 813, Loss 0.2335, LR 0.00000296\n",
      "Eval ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89179f06eeda4d8584af0c7ab2d4ebd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Step 5690, Val acc 0.8880, Val loss 0.0212\n",
      "Last checkpoint saved at ../exps/imagenet_m/last/checkpoint.pth\n",
      "Epoch 7, Batch 100, Loss 0.3260, LR 0.00000294\n",
      "Epoch 7, Batch 200, Loss 0.2781, LR 0.00000291\n",
      "Epoch 7, Batch 300, Loss 0.2744, LR 0.00000289\n",
      "Epoch 7, Batch 400, Loss 0.2884, LR 0.00000287\n",
      "Epoch 7, Batch 500, Loss 0.3076, LR 0.00000284\n",
      "Epoch 7, Batch 600, Loss 0.2894, LR 0.00000282\n",
      "Epoch 7, Batch 700, Loss 0.2754, LR 0.00000280\n",
      "Epoch 7, Batch 800, Loss 0.2740, LR 0.00000278\n",
      "Epoch 7, Batch 813, Loss 0.2405, LR 0.00000277\n",
      "Eval ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44624fe73234036901da50def8ebb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Step 6503, Val acc 0.8900, Val loss 0.0211\n",
      "Last checkpoint saved at ../exps/imagenet_m/last/checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "track = True\n",
    "# track = False\n",
    "\n",
    "if track:\n",
    "    import wandb\n",
    "    wandb.init(project='sop')\n",
    "    wandb.run.name = os.path.basename(exp_dir)\n",
    "\n",
    "# Iterate over the data\n",
    "best_val_acc = 0.0\n",
    "step = 0\n",
    "train_log_interval = 100\n",
    "val_eval_interval = 1000\n",
    "\n",
    "logging.basicConfig(filename=os.path.join(exp_dir, 'train.log'), level=logging.INFO)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_total = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs, mask_batch_size=mask_batch_size)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        running_total += labels.size(0)\n",
    "        \n",
    "        if i % train_log_interval == train_log_interval - 1 or i == len(train_dataloader) - 1:\n",
    "            # Print training loss every 100 batches\n",
    "            curr_lr = float(optimizer.param_groups[0]['lr'])\n",
    "            log_message = f'Epoch {epoch}, Batch {i + 1}, Loss {running_loss / running_total:.4f}, LR {curr_lr:.8f}'\n",
    "            print(log_message)\n",
    "            logging.info(log_message)\n",
    "            if track:\n",
    "                wandb.log({'train_loss': running_loss / running_total,\n",
    "                        'lr': curr_lr,\n",
    "                        'epoch': epoch,\n",
    "                        'step': step})\n",
    "            running_loss = 0.0\n",
    "            running_total = 0\n",
    "            \n",
    "        if i % val_eval_interval == val_eval_interval - 1 or i == len(train_dataloader) - 1:\n",
    "            val_results = eval(model, val_dataloader, criterion)\n",
    "            val_acc = val_results['val_acc']\n",
    "            val_loss = val_results['val_loss']\n",
    "            log_message = f'Epoch {epoch}, Step {step}, Val acc {val_acc:.4f}, Val loss {val_loss:.4f}'\n",
    "            print(log_message)\n",
    "            logging.info(log_message)\n",
    "            if track:\n",
    "                wandb.log({'val_acc': val_acc,\n",
    "                           'val_loss': val_loss,\n",
    "                        'epoch': epoch,\n",
    "                        'step': step})\n",
    "            \n",
    "            last_dir = os.path.join(exp_dir, 'last')\n",
    "            best_dir = os.path.join(exp_dir, 'best')\n",
    "            checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'step': step,\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_acc': val_acc,\n",
    "                }\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                os.makedirs(best_dir, exist_ok=True)\n",
    "                best_checkpoint_path = os.path.join(best_dir, 'checkpoint.pth')\n",
    "                torch.save(checkpoint, best_checkpoint_path)\n",
    "                config_best_checkpoint_path = os.path.join(best_dir, 'config.json')\n",
    "                config.save_to_json(config_best_checkpoint_path)\n",
    "                print(f'Best checkpoint saved at {best_checkpoint_path}')\n",
    "                \n",
    "                # model.save_pretrained(best_dir)\n",
    "            # model.save_pretrained(last_dir)\n",
    "            os.makedirs(last_dir, exist_ok=True)\n",
    "            last_checkpoint_path = os.path.join(last_dir, 'checkpoint.pth')\n",
    "            torch.save(checkpoint, last_checkpoint_path)\n",
    "            config_last_checkpoint_path = os.path.join(last_dir, 'config.json')\n",
    "            config.save_to_json(config_best_checkpoint_path)\n",
    "            print(f'Last checkpoint saved at {last_checkpoint_path}')\n",
    "            \n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "model.save(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69067c-a16b-4417-a3db-f4a304f520a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
